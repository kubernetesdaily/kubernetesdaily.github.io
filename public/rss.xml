<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Kubernetes Daily</title>
    <link>https://kubernetesdaily.github.io</link>
    <description>Learn Kubernetes, Cloud Native, DevOps, and Container technologies with hands-on tutorials, guides, and best practices.</description>
    <language>en</language>
    <lastBuildDate>2025-04-29T06:45:07.645Z</lastBuildDate>
    <atom:link href="https://kubernetesdaily.github.io/rss.xml" rel="self" type="application/rss+xml"/>

    <item>
      <title>undefined</title>
      <link>https://kubernetesdaily.github.io/blog/blog/werf</link>
      <guid isPermaLink="true">https://kubernetesdaily.github.io/blog/blog/werf</guid>
      <pubDate>Tue, 29 Apr 2025 06:45:07 GMT</pubDate>
      <description><![CDATA[title : Werf A solution for implementing efficient and consistent software

[![werf/werf](https://github-link-card.s3.ap-northeast-1.amazonaws.com/werf/werf.png)](https://github.com/werf/werf)

#### Install Werf 



```bash
➜  ~  curl -sSLO https://werf.io/install.sh &amp;&amp; chmod +x install.sh
➜  ~ ./install.sh --version 1.2 --channel stable
[INPUT REQUIRED] Current login shell is "zsh". Press ENTER to setup werf for this shell or choose another one.
[b]ash/[z]sh/[a]bort? Default: zsh.
z
[INPUT REQUIRED] trdl is going to be installed in "/Users/sangambiradar/bin/". Add this directory to your $PATH in "/Users/sangambiradar/.zshrc" and "/Users/sangambiradar/.zprofile"? (strongly recommended)
[y]es/[a]bort/[s]kip? Default: yes.
yes
[INFO] Installing trdl to "/Users/sangambiradar/bin/".

[INFO] Adding werf repo to trdl.
[INPUT REQUIRED] Add automatic werf activation to "/Users/sangambiradar/.zshrc" and "/Users/sangambiradar/.zprofile"? (recommended for interactive usage, not recommended for CI)
[y]es/[a]bort/[s]kip? Default: yes.
[INFO] werf installation finished successfully!
[INFO] Open new shell session if you have enabled werf autoactivation or activate werf manually with:
$ source $("/Users/sangambiradar/bin/trdl" use werf "1.2" "stable")
```

### werf use trdl 

trdl is an Open Source solution providing a secure channel for delivering updates from the Git repository to the end user. 


![](./trdl.png)

Problems that trdl solves for you
- Continuous delivery limitations
   - Continuous delivery via the CI system works well for the SaaS model, that is, for software 
   - hosted on cloud servers. At the same time, you would be hard-pressed to find a tool that is as   fast at delivering code to user devices.

- Challenges related to implementing a secure delivery
   - What complicates things:
     - Software release and changes in release channels must only occur based on a collective decision of the team (quorum).
     - The system must protect against unauthorized access and data compromise.
     - The system must not be compromised by human mistakes, including regulation breaches.

- Package manager limitations
  - There are many package managers, but they all have common drawbacks:
  - Each platform requires its own manager.
  - The package creation process is complicated.
  - There is a lot of manual work involved: the user has to add the package source, find the package, and install/upgrade/uninstall it.



### verify werf cli installed or not 

```
➜  ~ werf
werf helps to implement and support Continuous Integration and Continuous       
Delivery.

Find more information at https://werf.io

Delivery commands:
  converge        Build and push images, then deploy application into Kubernetes
  dismiss         Delete werf release from Kubernetes
  bundle          Work with werf bundles: publish bundles into container        
                  registry and deploy bundles into Kubernetes cluster

Cleaning commands:
  cleanup         Cleanup project images in the container registry
  purge           Purge all project images in the container registry

Helper commands:
  ci-env          Generate werf environment variables for specified CI system
  build           Build images
  export          Export images
  run             Run container for project image
  kube-run        Run container for project image in Kubernetes
  compose         Work with docker-compose
  slugify         Print slugged string by specified format.
  render          Render Kubernetes templates

Low-level management commands:
  config          Work with werf.yaml
  managed-images  Work with managed images which will be preserved during       
                  cleanup procedure
  host            Work with werf cache and data of all projects on the host     
                  machine
  helm            Manage application deployment with helm
  cr              Work with container registry: authenticate, list and remove   
                  images, etc.
  kubectl         kubectl controls the Kubernetes cluster manager

Other commands:
  synchronization Run synchronization server
  completion      Generate bash completion scripts
  version         Print version

Use "werf &lt;command&gt; --help" for more information about a given command.

Version: v1.2.205
➜  ~ 
```
### Setup Minikube for Werf 

```
 ~ minikube start --vm=true --insecure-registry registry.example.com:80
😄  minikube v1.30.0 on Darwin 13.3 (arm64)
✨  Automatically selected the qemu2 driver. Other choices: virtualbox, ssh
🌐  Automatically selected the builtin network
❗  You are using the QEMU driver without a dedicated network, which doesn't support `minikube service` &amp; `minikube tunnel` commands.
To try the dedicated network see: https://minikube.sigs.k8s.io/docs/drivers/qemu/#networking
💿  Downloading VM boot image ...
    &gt; minikube-v1.30.0-arm64.iso....:  65 B / 65 B []]></description>
      
    </item>
    <item>
      <title>undefined</title>
      <link>https://kubernetesdaily.github.io/blog/blog/qwen-coder-models</link>
      <guid isPermaLink="true">https://kubernetesdaily.github.io/blog/blog/qwen-coder-models</guid>
      <pubDate>Tue, 29 Apr 2025 06:45:07 GMT</pubDate>
      <description><![CDATA[]]></description>
      
    </item>
    <item>
      <title>undefined</title>
      <link>https://kubernetesdaily.github.io/blog/blog/kubernetes-tools-guide</link>
      <guid isPermaLink="true">https://kubernetesdaily.github.io/blog/blog/kubernetes-tools-guide</guid>
      <pubDate>Tue, 29 Apr 2025 06:45:07 GMT</pubDate>
      <description><![CDATA[]]></description>
      
    </item>
    <item>
      <title>undefined</title>
      <link>https://kubernetesdaily.github.io/blog/blog/k8sgpt</link>
      <guid isPermaLink="true">https://kubernetesdaily.github.io/blog/blog/k8sgpt</guid>
      <pubDate>Tue, 29 Apr 2025 06:45:07 GMT</pubDate>
      <description><![CDATA[ K8sgpt - Chatgpt base SRE 
k8sgpt chat gpt for trubleshooting 



### Install k8sgpt ai 

```
  ~ brew tap k8sgpt-ai/k8sgpt
==&gt; Tapping k8sgpt-ai/k8sgpt
Cloning into '/opt/homebrew/Library/Taps/k8sgpt-ai/homebrew-k8sgpt'...
remote: Enumerating objects: 86, done.
remote: Counting objects: 100% (86/86), done.
remote: Compressing objects: 100% (85/85), done.
remote: Total 86 (delta 51), reused 2 (delta 0), pack-reused 0
Receiving objects: 100% (86/86), 18.77 KiB | 6.26 MiB/s, done.
Resolving deltas: 100% (51/51), done.
Tapped 1 formula (14 files, 37.8KB).
➜  ~ brew install k8sgpt
==&gt; Fetching k8sgpt-ai/k8sgpt/k8sgpt
==&gt; Downloading https://github.com/k8sgpt-ai/k8sgpt/releases/download/v0.3.0/k8sgpt_Darwin_arm64.tar.gz
==&gt; Downloading from https://objects.githubusercontent.com/github-production-release-asset-2e65be/617152691/d9f59995-7f6
################################################################################################################# 100.0%
==&gt; Installing k8sgpt from k8sgpt-ai/k8sgpt
🍺  /opt/homebrew/Cellar/k8sgpt/0.3.0: 6 files, 55.5MB, built in 1 second
==&gt; Running `brew cleanup k8sgpt`...

```

### K8s filters list 

```
k8sgpt filters list
Active: 
&gt; Pod
&gt; PersistentVolumeClaim
&gt; StatefulSet
&gt; CronJob
&gt; Deployment
&gt; ReplicaSet
&gt; Service
&gt; Ingress
&gt; Node
Unused: 
&gt; NetworkPolicy
&gt; HorizontalPodAutoScaler
&gt; PodDisruptionBudget
 
```

### How K8sGPT works ?

K8sGPT uses analyzers to triage and diagnose issues in your cluster. It has a set of analyzers that are built in, but you will be able to write your own analyzers.

Built in analyzers

Enabled by default

- podAnalyzer 
  https://github.com/k8sgpt-ai/k8sgpt/blob/main/pkg/analyzer/pod.go
   - search all namespaces for pods that are not running
   - Check through container status to check for crashes or unready
   - a container that is still being created or blocked due to conditions such as OOMKilled
   - when pod is Running but its ReadinessProbe fails

- pvcAnalyzer  
   - PersistentVolumeClaim.ObjectMeta 
   - Error: value.FailureDetails
- rsAnalyzer
   - Status of replicaset 
   - type of event 
   - reason of failure 

  ```
   Events:
  Type    Reason            Age   From                   Message
  ]]></description>
      
    </item>
    <item>
      <title>undefined</title>
      <link>https://kubernetesdaily.github.io/blog/blog/cloudflared-tunnel</link>
      <guid isPermaLink="true">https://kubernetesdaily.github.io/blog/blog/cloudflared-tunnel</guid>
      <pubDate>Tue, 29 Apr 2025 06:45:07 GMT</pubDate>
      <description><![CDATA[]]></description>
      
    </item>
    <item>
      <title>undefined</title>
      <link>https://kubernetesdaily.github.io/blog/blog/cloud-native-learning-path</link>
      <guid isPermaLink="true">https://kubernetesdaily.github.io/blog/blog/cloud-native-learning-path</guid>
      <pubDate>Tue, 29 Apr 2025 06:45:07 GMT</pubDate>
      <description><![CDATA[]]></description>
      
    </item>
    <item>
      <title>undefined</title>
      <link>https://kubernetesdaily.github.io/blog/blog/arkade</link>
      <guid isPermaLink="true">https://kubernetesdaily.github.io/blog/blog/arkade</guid>
      <pubDate>Tue, 29 Apr 2025 06:45:07 GMT</pubDate>
      <description><![CDATA[
[![alexellis/arkade](https://github-link-card.s3.ap-northeast-1.amazonaws.com/alexellis/arkade.png)](https://github.com/alexellis/arkade)

### Getting started with Arkade CLI 

- Macos/Linux 

```bash

~ curl -sLS https://get.arkade.dev | sudo sh

Password:
Downloading package https://github.com/alexellis/arkade/releases/download/0.9.7/arkade-darwin-arm64 as /tmp/arkade-darwin-arm64
Download complete.

Running with sufficient permissions to attempt to move arkade to /usr/local/bin
New version of arkade installed to /usr/local/bin
Creating alias 'ark' for 'arkade'.
            _             _      
  __ _ _ __| | ____ _  __| | ___ 
 / _` | '__| |/ / _` |/ _` |/ _ \
| (_| | |  |   &lt; (_| | (_| |  __/
 \__,_|_|  |_|\_\__,_|\__,_|\___|

Open Source Marketplace For Developer Tools

Version: 0.9.7
Git Commit: 461fb7a9d05d7e3d13a39e03e1e38b6936cb15bd

 🐳 arkade needs your support: https://github.com/sponsors/alexellis
➜  ~ 

```

### Get list of most needed tools &amp; CLI for all Kubenetes Developer 

```bash
➜  ~ arkade get 
+]]></description>
      
    </item>
    <item>
      <title>undefined</title>
      <link>https://kubernetesdaily.github.io/blog/blog/Okteto</link>
      <guid isPermaLink="true">https://kubernetesdaily.github.io/blog/blog/Okteto</guid>
      <pubDate>Tue, 29 Apr 2025 06:45:07 GMT</pubDate>
      <description><![CDATA[ Okteto Develop your applications directly in your Kubernetes Cluster

[![okteto/okteto](https://github-link-card.s3.ap-northeast-1.amazonaws.com/okteto/okteto.png)](https://github.com/okteto/okteto)

### Getting started with Oketo CLI  

- Macos/Linux 

```bash

➜ curl https://get.okteto.com -sSfL | sh
&gt; Using Release Channel: stable
&gt; Using Version: 2.14.0
&gt; Downloading https://downloads.okteto.com/cli/stable/2.14.0/okteto-Darwin-arm64
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100 74.8M  100 74.8M    0     0  20.0M      0  0:00:03  0:00:03 --:--:-- 20.0M

```

### Login into Okteto Cloud via Github Account 

![](./okteto-login.png)


### Configuring Okteto CLI with Okteto Cloud

```bash

➜   okteto context
A context defines the default cluster/namespace for any Okteto CLI command.
Select the context you want to use:
Use the arrow keys to navigate: ↓ ↑ → ← 
  ▸ https://cloud.okteto.com (Okteto Cloud) *
    docker-desktop
    minikube
    multinode-pod-security
    
    Create new context
```
Or use directly 

```bash
➜ okteto context use https://cloud.okteto.com
 ✓  Using sangam14 @ cloud.okteto.com
```

2048 folder contain source code for 2048 

```Dockerfile
FROM nginx

COPY 2048 /usr/share/nginx/html

EXPOSE 80

CMD ["nginx", "-g", "daemon off;"]
```

```Docker-Compose.yml
services:
 game:
    build: ./
    expose:
      - 8080:80
```
### Deploy Docker Compose on Okteto Cloud 

okteto use [compose-spec](https://github.com/compose-spec/compose-spec) also its convert docker compose to kubernetes Manifest 

here is more about docker compose on okteto (https://www.okteto.com/docs/reference/compose/) 

```bash
okteto deploy --build 
 i  Using sangam14 @ cloud.okteto.com as context
 i  Building 'Dockerfile' in tcp://buildkit.cloud.okteto.net:443...
[+] Building 7.8s (7/7) FINISHED                                                                                          
 =&gt; [internal] load build definition from buildkit-3366967893                                                        0.9s
 =&gt; =&gt; transferring dockerfile: 137B                                                                                 0.8s
 =&gt; [internal] load .dockerignore                                                                                    0.7s
 =&gt; =&gt; transferring context: 2B                                                                                      0.6s
 =&gt; [internal] load metadata for docker.io/library/nginx:latest                                                      1.1s
 =&gt; [internal] load build context                                                                                    2.5s
 =&gt; =&gt; transferring context: 603.29kB                                                                                2.5s
 =&gt; CACHED [1/2] FROM docker.io/library/nginx@sha256:2ab30d6ac53580a6db8b657abf0f68d75360ff5cc1670a85acb5bd85ba1b19  0.0s
 =&gt; =&gt; resolve docker.io/library/nginx@sha256:2ab30d6ac53580a6db8b657abf0f68d75360ff5cc1670a85acb5bd85ba1b19c0       0.0s
 =&gt; [2/2] COPY 2048 /usr/share/nginx/html                                                                            0.3s
 =&gt; exporting to image                                                                                               2.5s
 =&gt; =&gt; exporting layers                                                                                              0.1s
 =&gt; =&gt; exporting manifest sha256:79f1385595d70b82b4b5db52f5edddb1f4a06c79ded928926d48bb947a5322b6                    0.0s
 =&gt; =&gt; exporting config sha256:ecf1dac7fbfaa6cdcb88dd2bbc616edbbc96459b078f15199fe3ddd564c481de                      0.0s
 =&gt; =&gt; pushing layers                                                                                                1.6s
 =&gt; =&gt; pushing manifest for registry.cloud.okteto.net/sangam14/okteto-2048-game:okteto@sha256:79f1385595d70b82b4b5d  0.8s
 ✓  Image 'registry.cloud.okteto.net/sangam14/okteto-2048-game:okteto' successfully pushed
 ✓  Kubernetes service 'game' created
 ✓  Endpoint 'game' created
 ✓  Service 'game' created
 ✓  Compose 'Okteto-2048' successfully deployed
 ✓  Development environment 'Okteto-2048' successfully deployed
 i  Run 'okteto up' to activate your development container

```

### Check it out Okteto Cloud 

Okteto basically use technic all 

![](./okteto-UI.png)


### Check it out enpoints 

you will see application running 

![](./okteto-2048-game.png)

### lets same app using kubernetes 

okteto also provide capablities to build docker images remotely its means without installing docker you can now build docker images .

```bash 

➜  okteto build -t sangam14/okteto-2048-game:okteto .
 i  Building 'Dockerfile' in tcp://buildkit.cloud.okteto.net:443...
[+] Building 5.9s (8/8) FINISHED                                                                                          
 =&gt; [internal] load build definition from buildkit-3724659360                                                        1.5s
 =&gt; =&gt; transferring dockerfile: 180B                                                                                 1.5s
 =&gt; [internal] load .dockerignore                                                                                    0.9s
 =&gt; =&gt; transferring context: 2B                                                                                      0.8s
 =&gt; [internal] load metadata for docker.io/library/nginx:latest                                                      0.9s
 =&gt; CACHED [1/3] FROM docker.io/library/nginx@sha256:2ab30d6ac53580a6db8b657abf0f68d75360ff5cc1670a85acb5bd85ba1b19  0.0s
 =&gt; =&gt; resolve docker.io/library/nginx@sha256:2ab30d6ac53580a6db8b657abf0f68d75360ff5cc1670a85acb5bd85ba1b19c0       0.0s
 =&gt; [internal] load build context                                                                                    0.9s
 =&gt; =&gt; transferring context: 4.66kB                                                                                  0.5s
 =&gt; [2/3] RUN chmod +x /usr/sbin/nginx                                                                               0.9s
 =&gt; [3/3] COPY 2048 /usr/share/nginx/html                                                                            0.3s
 =&gt; exporting to image                                                                                               2.1s
 =&gt; =&gt; exporting layers                                                                                              0.3s
 =&gt; =&gt; exporting manifest sha256:ad9bdd97413eddbcd25fe2d1c55992796848a314e33165235a307726dcadaf8a                    0.0s
 =&gt; =&gt; exporting config sha256:2f2ffc174a087bf9653029ec247279885a59e77887a259e592761ed4b06a3959                      0.0s
 =&gt; =&gt; pushing layers                                                                                                1.5s
 =&gt; =&gt; pushing manifest for docker.io/sangam14/okteto-2048-game:okteto@sha256:ad9bdd97413eddbcd25fe2d1c55992796848a  0.3s
 ✓  Image 'sangam14/okteto-2048-game:okteto' successfully pushed

```

### Okteto Support Kubernetes , Kustomize also  Helm charts 

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  selector:
    matchLabels:
      app: nginx
  replicas: 2
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: docker.io/sangam14/okteto-2048-game:okteto
        ports:
        - containerPort: 80
]]></description>
      
    </item>
    <item>
      <title>undefined</title>
      <link>https://kubernetesdaily.github.io/blog/blog/Flannel</link>
      <guid isPermaLink="true">https://kubernetesdaily.github.io/blog/blog/Flannel</guid>
      <pubDate>Tue, 29 Apr 2025 06:45:07 GMT</pubDate>
      <description><![CDATA[]]></description>
      
    </item>
    <item>
      <title>undefined</title>
      <link>https://kubernetesdaily.github.io/blog/blog/CertManager</link>
      <guid isPermaLink="true">https://kubernetesdaily.github.io/blog/blog/CertManager</guid>
      <pubDate>Tue, 29 Apr 2025 06:45:07 GMT</pubDate>
      <description><![CDATA[### CertManager - Automatically provision and manage TLS certificates in Kubernetes

[![cert-manager/cert-manager](https://github-link-card.s3.ap-northeast-1.amazonaws.com/cert-manager/cert-manager.png)](https://github.com/cert-manager/cert-manager)

### Start Minikube
```
minikube-certmanager git:(main) minikube start
😄  minikube v1.30.0 on Darwin 13.3.1 (arm64)
✨  Using the docker driver based on existing profile
👍  Starting control plane node minikube in cluster minikube
🚜  Pulling base image ...
🏃  Updating the running docker "minikube" container ...
❗  Image was not built for the current minikube version. To resolve this you can delete and recreate your minikube cluster using the latest images. Expected minikube version: v1.29.0 -&gt; Actual minikube version: v1.30.0
🐳  Preparing Kubernetes v1.26.3 on Docker 23.0.2 ...
🔎  Verifying Kubernetes components...
    ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
🌟  Enabled addons: storage-provisioner, default-storageclass
🏄  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default
```

### add jetstack helm 
```
➜  minikube-certmanager git:(main) helm repo add jetstack https://charts.jetstack.io  
"jetstack" has been added to your repositories
```

### update helm charts 
```
➜  minikube-certmanager git:(main) helm repo update
Hang tight while we grab the latest from your chart repositories...
...Successfully got an update from the "jetstack" chart repository
Update Complete. ⎈Happy Helming!⎈
```

### Install cert-manger CRD on test namespace 
```
➜  minikube-certmanager git:(main) helm install \
    cert-manager jetstack/cert-manager \
    --namespace test \
    --create-namespace \
    --version v1.11.1 \
    --set installCRDs=true
NAME: cert-manager
LAST DEPLOYED: Fri Apr 14 12:35:31 2023
NAMESPACE: test
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
cert-manager v1.11.1 has been deployed successfully!

In order to begin issuing certificates, you will need to set up a ClusterIssuer
or Issuer resource (for example, by creating a 'letsencrypt-staging' issuer).

More information on the different types of issuers and how to configure them
can be found in our documentation:

https://cert-manager.io/docs/configuration/

For information on how to configure cert-manager to automatically provision
Certificates for Ingress resources, take a look at the `ingress-shim`
documentation:

https://cert-manager.io/docs/usage/ingress/

```

### verify test namespace is active 

```
➜  minikube-certmanager git:(main) kubectl get ns
NAME              STATUS   AGE
default           Active   16h
kube-node-lease   Active   16h
kube-public       Active   16h
kube-system       Active   16h
test              Active   4m31s
```

### create self-signered certificate issuer  

creating a self-signed certificate that our CA will use. To do so we will first need to create a self-signed certificate issuer.


```yml
apiVersion: cert-manager.io/v1
kind: Issuer
metadata:
  name: selfsigned-issuer
  namespace: test
spec:
  selfSigned: {}
```
### kubectl apply cert manager ss issuer 

```
minikube-certmanager git:(main) ✗ kubectl create -f cert-manager-ss-issuer.yaml
issuer.cert-manager.io/selfsigned-issuer created
```
### creat CA certificate 


```
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: test-ca
  namespace: test
spec:
  isCA: true
  commonName: test-ca
  subject:
    organizations:
    - ACME Inc.
    organizationalUnits:
    - Widgets
  secretName: test-ca-secret
  privateKey:
    algorithm: ECDSA
    size: 256
  issuerRef:
    name: selfsigned-issuer
    kind: Issuer
    group: cert-manager.io

```

### kubectl apply  cert-manager-ca-cert 
```
➜  minikube-certmanager git:(main) ✗ kubectl create -f cert-manager-ca-cert.yaml
certificate.cert-manager.io/test-ca created
```
### checkt it out certificate 
```
➜  minikube-certmanager git:(main) ✗ kubectl -n test get certificate
NAME      READY   SECRET           AGE
test-ca   True    test-ca-secret   4m15s
```
### check it out secrets 
```
➜  minikube-certmanager git:(main) ✗ kubectl -n test get secret test-ca-secret
NAME             TYPE                DATA   AGE
test-ca-secret   kubernetes.io/tls   3      5m1s
```
Excellent! This secret contains the ca.crt, tls.crt, and tls.key that belong to the CA itself.

### create ca issuer 

Now it's time to create our CA issuer. Create a file called cert-manager-ca-issuer.yaml with the following:

```
apiVersion: cert-manager.io/v1
kind: Issuer
metadata:
  name: test-ca-issuer
  namespace: test
spec:
  ca:
    secretName: test-ca-secret
```

### apply ca issuer 
```
➜  minikube-certmanager git:(main) ✗ kubectl create -f cert-manager-ca-issuer.yaml 
issuer.cert-manager.io/test-ca-issuer created
```
### test ca cert 

```
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: test-server
  namespace: test
spec:
  secretName: test-server-tls
  isCA: false
  usages:
  - server auth
  - client auth
  dnsNames:
  - "test-server.test.svc.cluster.local"
  - "test-server"
  issuerRef:
    name: test-ca-issuer
]]></description>
      
    </item>
    <item>
      <title>undefined</title>
      <link>https://kubernetesdaily.github.io/labs/labs/Learn-k8s</link>
      <guid isPermaLink="true">https://kubernetesdaily.github.io/labs/labs/Learn-k8s</guid>
      <pubDate>Tue, 29 Apr 2025 06:45:07 GMT</pubDate>
      <description><![CDATA[## pre-requisites


####  Includes configuring pre-requisites to install kubeadm 

###### step 01) Enable following ports and protcols 

```
on kubemaster : Open following ports : https://kubernetes.io/docs/reference/networking/ports-and-protocols/

sudo ufw status
sudo ufw enable
sudo ufw allow 80/tcp
sudo ufw allow 443/tcp
sudo ufw allow 22/tcp
sudo ufw allow 6443/tcp
sudo ufw allow 2379/tcp
sudo ufw allow 2380/tcp
sudo ufw allow 10250/tcp
sudo ufw allow 10259/tcp
sudo ufw allow 10257/tcp
sudo ufw allow 30000:32767/tcp
sudo ufw reload
sudo ufw status

######  ]]></description>
      
    </item>
    <item>
      <title>undefined</title>
      <link>https://kubernetesdaily.github.io/labs/labs/Learn-Helm</link>
      <guid isPermaLink="true">https://kubernetesdaily.github.io/labs/labs/Learn-Helm</guid>
      <pubDate>Tue, 29 Apr 2025 06:45:07 GMT</pubDate>
      <description><![CDATA[

# Basics of Helm


git clone 


### Install Helm 3

```
  ~ curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100 11679  100 11679    0     0  32332      0 --:--:-- --:--:-- --:--:-- 32262
Downloading https://get.helm.sh/helm-v3.14.3-darwin-arm64.tar.gz
Verifying checksum... Done.
Preparing to install helm into /usr/local/bin
Password:
helm installed into /usr/local/bin/helm
```


### Creating the chart 

```
(base) ➜  helm-workshop git:(main) helm create application-1 
Creating application-1
```

### Structure of the chart 

```
(base) ➜  application-1 git:(main) ✗ tree
.
├── Chart.yaml
├── charts
├── templates
│   ├── NOTES.txt
│   ├── _helpers.tpl
│   ├── deployment.yaml
│   ├── hpa.yaml
│   ├── ingress.yaml
│   ├── service.yaml
│   ├── serviceaccount.yaml
│   └── tests
│       └── test-connection.yaml
└── values.yaml

4 directories, 10 files
(base) ➜  application-1 git:(main) ✗ 
```

deleted unwanted files 

```
base) ➜  application-1 git:(main) ✗ tree
.
├── Chart.yaml
├── charts
├── templates
│   ├── deployment.yaml
│   ├── service.yaml
│   └── tests
└── values.yaml
```

### Configuration the yamls files from scratch 

create ngnix deployment with 3 replicas and use nodeport to expose ports 



### how to deploy the chart 

```
(base) ➜  application-1 git:(main) ✗ helm install chart-1 .
W0323 05:55:21.671543    6164 warnings.go:70] unknown field "spec.ports[0].nodeport"
NAME: chart-1
LAST DEPLOYED: Sat Mar 23 05:55:21 2024
NAMESPACE: default
STATUS: deployed
REVISION: 1
TEST SUITE: None

### view the chart 
(base) ➜  application-1 git:(main) ✗ kubectl get deploy,svc
NAME                             READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/my-deployememt   0/1     1            0           49s

NAME                 TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
service/kubernetes   ClusterIP   10.96.0.1       &lt;none&gt;        443/TCP        23m
service/my-service   NodePort    10.102.58.218   &lt;none&gt;        80:31492/TCP   49s
(base) ➜  application-1 git:(main) ✗ kubectl get nodes -o wide 
NAME       STATUS   ROLES           AGE   VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION    CONTAINER-RUNTIME
minikube   Ready    control-plane   24m   v1.28.3   192.168.49.2   &lt;none&gt;        Ubuntu 22.04.3 LTS   6.6.16-linuxkit   docker://24.0.7


ip addess + port a

(base) ➜  application-1 git:(main) ✗ helm list 
NAME    NAMESPACE       REVISION        UPDATED                                 STATUS          CHART                   APP VERSION
chart-1 default         1               2024-03-23 05:55:21.559484 +0530 IST    deployed        application-1-0.1.0     1.16.0   
```
### delete the chart 

```
(base) ➜  application-1 git:(main) ✗ helm uninstall chart-1
release "chart-1" uninstalled
```

# Deep Dive into Charts


#### create new chart 

```
(base) ➜  helm-workshop git:(main) ✗ cd new-chart 
(base) ➜  new-chart git:(main) ✗ ls
Chart.yaml  charts      templates   values.yaml
(base) ➜  new-chart git:(main) ✗ 
```

### charts 

if you see your file structure you don't see charts folder 

```
.
├── Chart.yaml
├── charts
├── templates
│   ├── NOTES.txt
│   ├── _helpers.tpl
│   ├── deployment.yaml
│   ├── hpa.yaml
│   ├── ingress.yaml
│   ├── service.yaml
│   ├── serviceaccount.yaml
│   └── tests
│       └── test-connection.yaml
└── values.yaml

```

#### Notes.txt 

its container notes for user how to run this helm charts 

```
1. Get the application URL by running these commands:
{{- if .Values.ingress.enabled }}
{{- range $host := .Values.ingress.hosts }}
  {{- range .paths }}
  http{{ if $.Values.ingress.tls }}s{{ end }}://{{ $host.host }}{{ .path }}
  {{- end }}
{{- end }}
{{- else if contains "NodePort" .Values.service.type }}
  export NODE_PORT=$(kubectl get --namespace {{ .Release.Namespace }} -o jsonpath="{.spec.ports[0].nodePort}" services {{ include "new-chart.fullname" . }})
  export NODE_IP=$(kubectl get nodes --namespace {{ .Release.Namespace }} -o jsonpath="{.items[0].status.addresses[0].address}")
  echo http://$NODE_IP:$NODE_PORT
{{- else if contains "LoadBalancer" .Values.service.type }}
     NOTE: It may take a few minutes for the LoadBalancer IP to be available.
           You can watch the status of by running 'kubectl get --namespace {{ .Release.Namespace }} svc -w {{ include "new-chart.fullname" . }}'
  export SERVICE_IP=$(kubectl get svc --namespace {{ .Release.Namespace }} {{ include "new-chart.fullname" . }} --template "{{"{{ range (index .status.loadBalancer.ingress 0) }}{{.}}{{ end }}"}}")
  echo http://$SERVICE_IP:{{ .Values.service.port }}
{{- else if contains "ClusterIP" .Values.service.type }}
  export POD_NAME=$(kubectl get pods --namespace {{ .Release.Namespace }} -l "app.kubernetes.io/name={{ include "new-chart.name" . }},app.kubernetes.io/instance={{ .Release.Name }}" -o jsonpath="{.items[0].metadata.name}")
  export CONTAINER_PORT=$(kubectl get pod --namespace {{ .Release.Namespace }} $POD_NAME -o jsonpath="{.spec.containers[0].ports[0].containerPort}")
  echo "Visit http://127.0.0.1:8080 to use your application"
  kubectl --namespace {{ .Release.Namespace }} port-forward $POD_NAME 8080:$CONTAINER_PORT
{{- end }}

```


#### _helper

this file nothing to do with kubernetes resource its just variable 

```
{{/*
Expand the name of the chart.
*/}}
{{- define "new-chart.name" -}}
{{- default .Chart.Name .Values.nameOverride | trunc 63 | trimSuffix "-" }}
{{- end }}

{{/*
Create a default fully qualified app name.
We truncate at 63 chars because some Kubernetes name fields are limited to this (by the DNS naming spec).
If release name contains chart name it will be used as a full name.
*/}}
{{- define "new-chart.fullname" -}}
{{- if .Values.fullnameOverride }}
{{- .Values.fullnameOverride | trunc 63 | trimSuffix "-" }}
{{- else }}
{{- $name := default .Chart.Name .Values.nameOverride }}
{{- if contains $name .Release.Name }}
{{- .Release.Name | trunc 63 | trimSuffix "-" }}
{{- else }}
{{- printf "%s-%s" .Release.Name $name | trunc 63 | trimSuffix "-" }}
{{- end }}
{{- end }}
{{- end }}

{{/*
Create chart name and version as used by the chart label.
*/}}
{{- define "new-chart.chart" -}}
{{- printf "%s-%s" .Chart.Name .Chart.Version | replace "+" "_" | trunc 63 | trimSuffix "-" }}
{{- end }}

{{/*
Common labels
*/}}
{{- define "new-chart.labels" -}}
helm.sh/chart: {{ include "new-chart.chart" . }}
{{ include "new-chart.selectorLabels" . }}
{{- if .Chart.AppVersion }}
app.kubernetes.io/version: {{ .Chart.AppVersion | quote }}
{{- end }}
app.kubernetes.io/managed-by: {{ .Release.Service }}
{{- end }}

{{/*
Selector labels
*/}}
{{- define "new-chart.selectorLabels" -}}
app.kubernetes.io/name: {{ include "new-chart.name" . }}
app.kubernetes.io/instance: {{ .Release.Name }}
{{- end }}

{{/*
Create the name of the service account to use
*/}}
{{- define "new-chart.serviceAccountName" -}}
{{- if .Values.serviceAccount.create }}
{{- default (include "new-chart.fullname" .) .Values.serviceAccount.name }}
{{- else }}
{{- default "default" .Values.serviceAccount.name }}
{{- end }}
{{- end }}

```

### chart.yaml


its container helm chart versioning 

apiVersion: v2
name: new-chart
description: A Helm chart for Kubernetes

# A chart can be either an 'application' or a 'library' chart.
#
# Application charts are a collection of templates that can be packaged into versioned archives
# to be deployed.
#
# Library charts provide useful utilities or functions for the chart developer. They're included as
# a dependency of application charts to inject those utilities and functions into the rendering
# pipeline. Library charts do not define any templates and therefore cannot be deployed.
type: application

# This is the chart version. This version number should be incremented each time you make changes
# to the chart and its templates, including the app version.
# Versions are expected to follow Semantic Versioning (https://semver.org/)
version: 0.1.0

# This is the version number of the application being deployed. This version number should be
# incremented each time you make changes to the application. Versions are not expected to
# follow Semantic Versioning. They should reflect the version the application is using.
# It is recommended to use it with quotes.
appVersion: "1.16.0"

```

# Working with Multiple Values


#### create another values file 

```
# Default values for application-1.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.
deployment:
  replicaCount: 1
  name: new-deployememt
  image:
    app: ngnix 
    version: latest

service:
  name: my-service
  type: NodePort
  port: 80
  targetPort: 80
  nodePort: 32046
  selector:
    app: ngnix   

```

just chamged nortport value here 

#### install chart again 


```
base) ➜  application-1 git:(main) ✗ helm install chart-1 .
W0325 04:26:44.306101    7525 warnings.go:70] unknown field "spec.ports[0].nodeport"
NAME: chart-1
LAST DEPLOYED: Mon Mar 25 04:26:44 2024
NAMESPACE: default
STATUS: deployed
REVISION: 1
TEST SUITE: None


(base) ➜  application-1 git:(main) ✗ helm list  
NAME    NAMESPACE       REVISION        UPDATED                                 STATUS          CHART                     APP VERSION
chart-1 default         1               2024-03-25 04:26:44.235604 +0530 IST    deployed        application-1-0.1.0       1.16.0     
```

### deploy new chart 

```
➜  application-1 git:(main) ✗ helm install chart-2 . -f new-values.yaml 
W0325 04:31:02.212820    7835 warnings.go:70] unknown field "spec.ports[0].nodeport"
NAME: chart-2
LAST DEPLOYED: Mon Mar 25 04:31:02 2024
NAMESPACE: default
STATUS: deployed
REVISION: 1
TEST SUITE: None

```

## creating template file and access through it 


### create helper template file 

create file with name _my-template.tpl

```
{{- define "labels" }}
app: ngnix
version: "1.0"
team: myteam
{{- end }}
```

```
(base) ➜  application-1 git:(main) ✗ helm template .
]]></description>
      
    </item>
    <item>
      <title>undefined</title>
      <link>https://kubernetesdaily.github.io/labs/labs/Learn-Docker</link>
      <guid isPermaLink="true">https://kubernetesdaily.github.io/labs/labs/Learn-Docker</guid>
      <pubDate>Tue, 29 Apr 2025 06:45:07 GMT</pubDate>
      <description><![CDATA[

# Docker Prerequisites




&gt; Here are the steps to create a Docker Hub account:

1. Go to https://hub.docker.com/signup and click on the "Sign Up" button.
&lt;br&gt;
2. Fill out the registration form with your name, email address, and password.
&lt;br&gt;
3. Agree to the terms of service and privacy policy by checking the box.
&lt;br&gt;
4. Click on the "Sign Up" button to complete the registration process.
&lt;br&gt;
5. You will receive a verification email from Docker Hub. Follow the link in the email to verify your email address.
&lt;br&gt;

6. Once your email address is verified, you can log in to Docker Hub using your email address and password.
&lt;br&gt;

You can now create and manage your repositories, and upload your Docker images to share with the community.
That's it! You now have a Docker Hub account and you can start using it to store, share, and distribute your Docker images.


# Hello World in Docker


#### run your first hello world example

```sh
docker run hello-world
```
is a command that runs a simple Docker container to verify that Docker is correctly installed on your system and working as expected.

When you run this command, Docker will first check if the "hello-world" image is available locally. If the image is not found, Docker will download it from the Docker Hub registry.

Once the "hello-world" image is available, Docker will create a container from the image and run it. The container will print a message to the console to indicate that everything is working correctly.

Here's an example of what you might see when you run docker run hello-world:

```sh
docker run hello-world
Unable to find image 'hello-world:latest' locally
latest: Pulling from library/hello-world
7050e35b49f5: Pull complete 
Digest: sha256:6e8b6f026e0b9c419ea0fd02d3905dd0952ad1feea67543f525c73a0a790fefb
Status: Downloaded newer image for hello-world:latest

Hello from Docker!
This message shows that your installation appears to be working correctly.

To generate this message, Docker took the following steps:
 1. The Docker client contacted the Docker daemon.
 2. The Docker daemon pulled the "hello-world" image from the Docker Hub.
    (arm64v8)
 3. The Docker daemon created a new container from that image which runs the
    executable that produces the output you are currently reading.
 4. The Docker daemon streamed that output to the Docker client, which sent it
    to your terminal.

To try something more ambitious, you can run an Ubuntu container with:
 $ docker run -it ubuntu bash

Share images, automate workflows, and more with a free Docker ID:
 https://hub.docker.com/

For more examples and ideas, visit:
 https://docs.docker.com/get-started/


```
#### check it out list of docker images 

```sh
dockerworkshop git:(main) ✗ docker images
REPOSITORY                                TAG               IMAGE ID       CREATED         SIZE
hello-world                               latest            46331d942d63   11 months ago   9.14kB
```


docker inspect is a command used to retrieve detailed information about one or more Docker objects, such as containers, images, networks, volumes, and more. The command allows you to inspect the configuration and state of a Docker object, including its metadata, networking information, storage configuration, and more.

Here's the basic syntax of the docker inspect command:

```sh
docker inspect [OPTIONS] OBJECT [OBJECT...]

```
- OPTIONS: Optional flags that modify the output of the command.
- OBJECT: The name or ID of the Docker object to inspect.

For example, to inspect a running Docker container named hello-world , you could use the following command:

```sh
docker inspect hello-world                         
[
    {
        "Id": "sha256:46331d942d6350436f64e614d75725f6de3bb5c63e266e236e04389820a234c4",
        "RepoTags": [
            "hello-world:latest"
        ],
        "RepoDigests": [
            "hello-world@sha256:6e8b6f026e0b9c419ea0fd02d3905dd0952ad1feea67543f525c73a0a790fefb"
        ],
        "Parent": "",
        "Comment": "",
        "Created": "2022-03-19T16:12:58.923371954Z",
        "Container": "b2af51419cbf516f3c99b877a64906b21afedc175bd3cd082eb5798e2f277bb4",
        "ContainerConfig": {
            "Hostname": "b2af51419cbf",
            "Domainname": "",
            "User": "",
            "AttachStdin": false,
            "AttachStdout": false,
            "AttachStderr": false,
            "Tty": false,
            "OpenStdin": false,
            "StdinOnce": false,
            "Env": [
                "PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"
            ],
            "Cmd": [
                "/bin/sh",
                "-c",
                "#(nop) ",
                "CMD [\"/hello\"]"
            ],
            "Image": "sha256:cc0fff24c4ece63ade5d9f549e42c926cf569112c4f5c439a4a57f3f33f5588b",
            "Volumes": null,
            "WorkingDir": "",
            "Entrypoint": null,
            "OnBuild": null,
            "Labels": {}
        },
        "DockerVersion": "20.10.12",
        "Author": "",
        "Config": {
            "Hostname": "",
            "Domainname": "",
            "User": "",
            "AttachStdin": false,
            "AttachStdout": false,
            "AttachStderr": false,
            "Tty": false,
            "OpenStdin": false,
            "StdinOnce": false,
            "Env": [
                "PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"
            ],
            "Cmd": [
                "/hello"
            ],
            "Image": "sha256:cc0fff24c4ece63ade5d9f549e42c926cf569112c4f5c439a4a57f3f33f5588b",
            "Volumes": null,
            "WorkingDir": "",
            "Entrypoint": null,
            "OnBuild": null,
            "Labels": null
        },
        "Architecture": "arm64",
        "Variant": "v8",
        "Os": "linux",
        "Size": 9136,
        "VirtualSize": 9136,
        "GraphDriver": {
            "Data": {
                "MergedDir": "/var/lib/docker/overlay2/851a7de3abc0e1977e00c9bd8976c5fa1b0d954d3dc847ae15b36539f43e90a3/merged",
                "UpperDir": "/var/lib/docker/overlay2/851a7de3abc0e1977e00c9bd8976c5fa1b0d954d3dc847ae15b36539f43e90a3/diff",
                "WorkDir": "/var/lib/docker/overlay2/851a7de3abc0e1977e00c9bd8976c5fa1b0d954d3dc847ae15b36539f43e90a3/work"
            },
            "Name": "overlay2"
        },
        "RootFS": {
            "Type": "layers",
            "Layers": [
                "sha256:efb53921da3394806160641b72a2cbd34ca1a9a8345ac670a85a04ad3d0e3507"
            ]
        },
        "Metadata": {
            "LastTagTime": "0001-01-01T00:00:00Z"
        }
    }
]

```


# Docker Image Filtering


running `docker run alpine ` command would download the Alpine Linux image from Docker Hub and start a new container based on that image. You can then use the container to run commands or applications. When you exit the container, it will stop running.

Alpine Linux is a lightweight Linux distribution that is commonly used in Docker containers due to its small size and security features.

```sh
dockerworkshop git:(main) ✗ docker pull alpine:3.6 
docker pull alpine:3.7
docker pull alpine:3.8
docker pull alpine:3.9
3.6: Pulling from library/alpine
e8f81692e76c: Pull complete 
Digest: sha256:66790a2b79e1ea3e1dabac43990c54aca5d1ddf268d9a5a0285e4167c8b24475
Status: Downloaded newer image for alpine:3.6
docker.io/library/alpine:3.6
3.7: Pulling from library/alpine
40223db5366f: Pull complete 
Digest: sha256:8421d9a84432575381bfabd248f1eb56f3aa21d9d7cd2511583c68c9b7511d10
Status: Downloaded newer image for alpine:3.7
docker.io/library/alpine:3.7
3.8: Pulling from library/alpine
788aef77d06b: Pull complete 
Digest: sha256:2bb501e6173d9d006e56de5bce2720eb06396803300fe1687b58a7ff32bf4c14
Status: Downloaded newer image for alpine:3.8
docker.io/library/alpine:3.8
3.9: Pulling from library/alpine
941f399634ec: Pull complete 
Digest: sha256:414e0518bb9228d35e4cd5165567fb91d26c6a214e9c95899e1e056fcd349011
Status: Downloaded newer image for alpine:3.9
docker.io/library/alpine:3.9

```

#### docker images filtering 

The docker images command allows you to filter Docker images based on various criteria using the --filter option. Here are some common filters that you can use with the docker images command:


```sh
docker images --filter=reference='alpine'
REPOSITORY   TAG       IMAGE ID       CREATED       SIZE
alpine       3.9       9afdd4a290bf   2 years ago   5.3MB
alpine       3.8       b22edbe95d11   3 years ago   4.2MB
alpine       3.7       bd812700d284   3 years ago   4.01MB
alpine       3.6       25e814211fdd   3 years ago   3.84MB
```
#### Filters images that are or are not "dangling," meaning they are not tagged and not referenced by any container.

```sh
docker images --filter dangling=false 
```
Or to list images created before a specific image, you can run:

```sh
docker images --filter before=alpine:3.8 

```

# Images as Tar Files


#### Images and Container as Tarfile 

Docker provides the ability to save images and containers as tar files, which can be useful for sharing with others or transferring between systems. Here's how to do it:


```sh
docker pull nginx:latest
latest: Pulling from library/nginx
5731adb3a4ab: Pull complete 
8785c8f663d3: Pull complete 
023b6bd393e4: Pull complete 
fd8f86b165b0: Pull complete 
8f41e7c12976: Pull complete 
3b5338ea7d08: Pull complete 
Digest: sha256:6650513efd1d27c1f8a5351cbd33edf85cc7e0d9d0fcb4ffb23d8fa89b601ba8
Status: Downloaded newer image for nginx:latest
docker.io/library/nginx:latest

```

#### Saving an Image as a Tar File

To save a Docker image as a tar file, use the docker save command with the image name and output file name:

```sh
dockerworkshop git:(main) ✗ docker container run -it ubuntu:14.04 bash
Unable to find image 'ubuntu:14.04' locally
14.04: Pulling from library/ubuntu
d1a5a1e51f25: Pull complete 
75f8eea31a63: Pull complete 
a72d031efbfb: Pull complete 
Digest: sha256:64483f3496c1373bfd55348e88694d1c4d0c9b660dee6bfef5e12f43b9933b30
Status: Downloaded newer image for ubuntu:14.04
root@906d9f72e9fe:/# exit
exit

```

```sh
➜  dockerworkshop git:(main) ✗ docker ps
CONTAINER ID   IMAGE                       COMMAND                  CREATED             STATUS             PORTS     NAMES
1bf183201392   ubuntu:14.04                "bash"                   15 seconds ago      Up 14 seconds                loving_ride
```

```sh
docker export 1b  &gt; os.tar
docker export loving_ride  &gt; os1.tar
dockerworkshop git:(main) ✗ ls
os.tar   os1.tar
```


docker load is a command used to load images or container archives that were previously saved using the docker save command.

When you use the docker save command, it creates a tar archive of one or more Docker images and/or containers. You can then use the docker load command to load this tar archive back into Docker.

The syntax for using the docker load command is as follows:

```sh
➜  dockerworkshop git:(main) ✗ docker save -o os.tar ubuntu  
➜  dockerworkshop git:(main) ✗ docker load &lt; os.tar            
Loaded image: ubuntu:14.04
```


# Pushing to DockerHub


#### Pull nginx image from dockerhub using 

```sh
dockerworkshop git:(main) ✗ docker pull nginx
Using default tag: latest
latest: Pulling from library/nginx
Digest: sha256:6650513efd1d27c1f8a5351cbd33edf85cc7e0d9d0fcb4ffb23d8fa89b601ba8
Status: Image is up to date for nginx:latest
docker.io/library/nginx:latest
```
#### Run Docker with ngnix 

```sh
 dockerworkshop git:(main) ✗ docker run --name docker-nginx -p 80:80 -d nginx
63258aebdc2d8ea40a0099efb3e51f8b15db2fe2dc048da3901843b4782d19fb
```
–name docker-nginx : Name given to the container that is run is docker-nginx-p 80:80 : the port we are exposing and mapping from local machine port number to that of container, in the format local_machine_port:container_port-d : Detached mode – Runs the container in background

#### check all running docker containers 

```sh
➜  dockerworkshop git:(main) ✗ docker ps
CONTAINER ID   IMAGE                       COMMAND                  CREATED          STATUS          PORTS                NAMES
63258aebdc2d   nginx                       "/docker-entrypoint.…"   55 seconds ago   Up 55 seconds   0.0.0.0:80-&gt;80/tcp   docker-nginx
```

#### open localhost with specific port 

![](./images/ngnix.png)

#### Include a static Web Application in the Docker with NGINX

To include our static Web Application into the Docker Image with NGINX, we shall create a Dockerfile (including commands to build image) and an html file with name index.html (acting as our web application) in a directory named nginx-app.

create dockerfile with following content :

```dockerfile

FROM nginx
COPY . /usr/share/nginx/html

```
### create index.html file with following content 

```html

&lt;html&gt;
  &lt;head&gt;
    &lt;title&gt;Docker NGINX Tutorial&lt;/title&gt;
  &lt;/head&gt;
  &lt;body&gt;
    &lt;h1&gt;Join CloudNativeFolks Community&lt;/h1&gt;
    &lt;p&gt;Learn to Dockerize with NGINX and your web application.&lt;/p&gt;
    &lt;a href=" https://discord.com/invite/9ERSnT7 "&gt;Join Discord &lt;/a&gt;
    &lt;a href=" "&gt;NGINX Tutorial&lt;/a&gt;
  &lt;/body&gt;
&lt;/html&gt;

```

#### Build Dockerfile 

```sh
Dockerfile git:(main) ✗ docker build -t nginx-application -f dockerfile.ngnix .
[+] Building 0.1s (7/7) FINISHED                                                                                                                                                                  
 =&gt; [internal] load build definition from dockerfile.ngnix                                                                                                                                   0.0s
 =&gt; =&gt; transferring dockerfile: 87B                                                                                                                                                          0.0s
 =&gt; [internal] load .dockerignore                                                                                                                                                            0.0s
 =&gt; =&gt; transferring context: 2B                                                                                                                                                              0.0s
 =&gt; [internal] load metadata for docker.io/library/nginx:latest                                                                                                                              0.0s
 =&gt; [internal] load build context                                                                                                                                                            0.0s
 =&gt; =&gt; transferring context: 82B                                                                                                                                                             0.0s
 =&gt; [1/2] FROM docker.io/library/nginx                                                                                                                                                       0.0s
 =&gt; [2/2] COPY . /usr/share/nginx/html                                                                                                                                                       0.0s
 =&gt; exporting to image                                                                                                                                                                       0.0s
 =&gt; =&gt; exporting layers                                                                                                                                                                      0.0s
 =&gt; =&gt; writing image sha256:54027a144afd33ddd1449b757581c7b554d5411c4b2bac291f5dfbccb85fda41                                                                                                 0.0s
 =&gt; =&gt; naming to docker.io/library/nginx-application                                                                                                                                         0.0s
➜  Dockerfile git:(main) ✗ 
```


#### run updated ngnix webapp 

```sh
docker run --name docker-nginx-app  -p 80:80 -d nginx-application

```

#### List docker images 

```sh
Dockerfile git:(main) ✗ docker images
REPOSITORY                                                TAG                                                                          IMAGE ID       CREATED         SIZE
nginx-application                                         latest                                                                       e8742ef897ea   2 minutes ago   135MB
```

#### Tag Docker Images 

```sh
docker tag nginx-application  sangam14/nginx-application 

```

#### Login into Your DockerHub Account 

```sh
docker login 
docker push sangam14/nginx-application  
Using default tag: latest
The push refers to repository [docker.io/sangam14/nginx-application]
4e9e8987d0ed: Pushed 
7a99131e1da4: Mounted from library/nginx 
c61a83b92ad9: Mounted from library/nginx 
0d96feb871c8: Mounted from library/nginx 
902b28ccafe7: Mounted from library/nginx 
3063fc92629d: Mounted from library/nginx 
a49c6ceb5b3a: Mounted from library/nginx 
latest: digest: sha256:09f29db6e4179bd1019a48d2d50944989347fdf145193f4165353d5148a902c8 size: 1777

```

# Building a Base Image


#### write simple c program 

```c
#include&lt;stdio.h&gt;

int main()
{
printf("dockerworkshop");
}
```

#### Compile C program 

```sh
gcc -o hello hello.c 

✗ ./hello 
dockerworkshop%     

```

#### create dockerfile with following content : 


```dockerfile
FROM scratch
ADD hello /
CMD ["/hello"]
```

### Build Dockerfile without any base image 

```sh
Dockerfile git:(main) ✗ docker build -t sangam14/hello-scratch -f dockerfile.hello .
[+] Building 0.1s (5/5) FINISHED                                                                                                                                                                  
 =&gt; [internal] load build definition from dockerfile.hello                                                                                                                                   0.0s
 =&gt; =&gt; transferring dockerfile: 87B                                                                                                                                                          0.0s
 =&gt; [internal] load .dockerignore                                                                                                                                                            0.0s
 =&gt; =&gt; transferring context: 2B                                                                                                                                                              0.0s
 =&gt; [internal] load build context                                                                                                                                                            0.0s
 =&gt; =&gt; transferring context: 33.47kB                                                                                                                                                         0.0s
 =&gt; [1/1] ADD hello /                                                                                                                                                                        0.0s
 =&gt; exporting to image                                                                                                                                                                       0.0s
 =&gt; =&gt; exporting layers                                                                                                                                                                      0.0s
 =&gt; =&gt; writing image sha256:769934a6858c0910a3682e966da6c8d9c15b0324307b092eb77258a9a08879ce                                                                                                 0.0s
 =&gt; =&gt; naming to docker.io/sangam14/hello-scratch       

 ```

#### run docker image 

```sh
 docker run sangam14/hello-scratch 
 dockerworkshop

```


# Dockerfile ADD


Here's an example of a Dockerfile that uses the ADD instruction to copy a local file into a Docker image:


```Dockerfile
FROM ubuntu:latest

WORKDIR /app

ADD example.txt /app/

CMD ["cat", "/app/example.txt"]

```

#### Build Dockerfile 


```bash

 Dockerfile git:(main) ✗ docker build -t sangam14/add-dockerfile  -f dockerfile.add .  
[+] Building 5.2s (9/9) FINISHED                                                                                                                                                                  
 =&gt; [internal] load build definition from dockerfile.add                                                                                                                                     0.0s
 =&gt; =&gt; transferring dockerfile: 131B                                                                                                                                                         0.0s
 =&gt; [internal] load .dockerignore                                                                                                                                                            0.0s
 =&gt; =&gt; transferring context: 2B                                                                                                                                                              0.0s
 =&gt; [internal] load metadata for docker.io/library/ubuntu:latest                                                                                                                             3.1s
 =&gt; [auth] library/ubuntu:pull token for registry-1.docker.io                                                                                                                                0.0s
 =&gt; [1/3] FROM docker.io/library/ubuntu:latest@sha256:9a0bdde4188b896a372804be2384015e90e3f84906b750c1a53539b585fbbe7f                                                                       2.0s
 =&gt; =&gt; resolve docker.io/library/ubuntu:latest@sha256:9a0bdde4188b896a372804be2384015e90e3f84906b750c1a53539b585fbbe7f                                                                       0.0s
 =&gt; =&gt; sha256:9a0bdde4188b896a372804be2384015e90e3f84906b750c1a53539b585fbbe7f 1.13kB / 1.13kB                                                                                               0.0s
 =&gt; =&gt; sha256:61bd0b97000996232eb07b8d0e9375d14197f78aa850c2506417ef995a7199a7 424B / 424B                                                                                                   0.0s
 =&gt; =&gt; sha256:a6be1f66f70f66ef43503292e38ccbfc14f2d5464e7736344783a8fc7bb339a8 2.31kB / 2.31kB                                                                                               0.0s
 =&gt; =&gt; sha256:8b150fd943bcd54ef788cece17523d19031f745b099a798de65247900d102e18 27.34MB / 27.34MB                                                                                             1.4s
 =&gt; =&gt; extracting sha256:8b150fd943bcd54ef788cece17523d19031f745b099a798de65247900d102e18                                                                                                    0.4s
 =&gt; [internal] load build context                                                                                                                                                            0.0s
 =&gt; =&gt; transferring context: 78B                                                                                                                                                             0.0s
 =&gt; [2/3] WORKDIR /app                                                                                                                                                                       0.1s
 =&gt; [3/3] ADD example.txt /app/                                                                                                                                                              0.0s
 =&gt; exporting to image                                                                                                                                                                       0.0s
 =&gt; =&gt; exporting layers                                                                                                                                                                      0.0s
 =&gt; =&gt; writing image sha256:c3438bfac421fa098b47f37ae00427eadcfb7ed36653a678738c63b0ab33a8d1                                                                                                 0.0s
 =&gt; =&gt; naming to docker.io/sangam14/add-dockerfile               


 ```

 ### run docker images 

```
Dockerfile git:(main) ✗ docker run sangam14/add-dockerfile                                                                                                                                     
Sangam Biradar 
Docker Community Leader 

```


# Dockerfile COPY


COPY is a dockerfile command that copies files from a local source location to a destination in the Docker container. A Dockerfile is a text file with instructions to set up a Docker container.

create myfile1.txt and myfile2.txt with following content :

```sh
# myfile1.txt
Hello This is my first file !
This is file will be copied in /usr/share directory from Docker host to Docker Container.
```

```sh
# myfile2.txt
Hello This is my second file !
This is file will be copied in /tmp directory from Docker host to Docker Container.

```
The general syntax of the COPY command is:

```sh
COPY &lt;src&gt; &lt;dest&gt;

```
Here, `&lt;src&gt;` and `&lt;dest&gt;`are file paths.` &lt;src&gt;` is the path to the source folder containing files to be copied. This option can be left empty to copy the contents of the current directory. The source of the files has to be a directory on the local computer.

`&lt;dest&gt;` is the destination of the COPY command inside the docker container. This is the path where files are to be copied.

```Dockerfile
# Instruction for Dockerfile to create a new image on top of the base image (ubuntu)
# Using the base image ubuntu: latest
FROM ubuntu:latest
# Copying myfile1.txt to the containers /usr/share directory
COPY myfile1.txt /usr/share
# Copying myfile2.txt to the containers /tmp directory
COPY myfile2.txt /tmp

```
### Build Dockerfile using following Command 

```sh
  Dockerfile git:(main) ✗ docker build -t sangam14/copy-dockerfile  -f dockerfile.copy .
[+] Building 2.2s (9/9) FINISHED                                                                                                                                                                  
 =&gt; [internal] load build definition from dockerfile.copy                                                                                                                                    0.0s
 =&gt; =&gt; transferring dockerfile: 356B                                                                                                                                                         0.0s
 =&gt; [internal] load .dockerignore                                                                                                                                                            0.0s
 =&gt; =&gt; transferring context: 2B                                                                                                                                                              0.0s
 =&gt; [internal] load metadata for docker.io/library/ubuntu:latest                                                                                                                             2.1s
 =&gt; [auth] library/ubuntu:pull token for registry-1.docker.io                                                                                                                                0.0s
 =&gt; CACHED [1/3] FROM docker.io/library/ubuntu:latest@sha256:9a0bdde4188b896a372804be2384015e90e3f84906b750c1a53539b585fbbe7f                                                                0.0s
 =&gt; [internal] load build context                                                                                                                                                            0.0s
 =&gt; =&gt; transferring context: 344B                                                                                                                                                            0.0s
 =&gt; [2/3] COPY myfile1.txt /usr/share                                                                                                                                                        0.0s
 =&gt; [3/3] COPY myfile2.txt /tmp                                                                                                                                                              0.0s
 =&gt; exporting to image                                                                                                                                                                       0.0s
 =&gt; =&gt; exporting layers                                                                                                                                                                      0.0s
 =&gt; =&gt; writing image sha256:4c660d66bd5f94311a22be23394032e2f2dd45f40fb4831f8e083efe90488763                                                                                                 0.0s
 =&gt; =&gt; naming to docker.io/sangam14/copy-dockerfile    

 ```

 #### check inside container and search for text file 

```sh
➜  Dockerfile git:(main) ✗ docker run -it sangam14/copy-dockerfile bash                  
root@27a3fbe098c3:/# ls
bin  boot  dev  etc  home  lib  media  mnt  opt  proc  root  run  sbin  srv  sys  tmp  usr  var
root@27a3fbe098c3:/# cat /usr/share/myfile1.txt 
# myfile1.txt
Hello This is my first file !
This is file will be copied in /usr/share directory from Docker host to Docker Container.root@27a3fbe098c3:/# ls
bin  boot  dev  etc  home  lib  media  mnt  opt  proc  root  run  sbin  srv  sys  tmp  usr  var
root@27a3fbe098c3:/# cat /tmp/myfile2.txt 

# myfile2.txt
Hello This is my second file !
This is file will be copied in /tmp directory from Docker host to Docker Container.root@27a3fbe098c3:/# 

```
both file successfully copied inside container 



# Dockerfile CMD 



The CMD command we saw earlier followed the Shell syntax:

```bash
CMD executable parameter1 parameter2
```

However, it is better practice to use the JSON array format:

```json
CMD ["executable", "parameter1", "parameter2"]
```

A CMD command can be overridden by providing the executable and its parameters in the docker ​run command. For example:


```dockerfile 
FROM ubuntu
RUN apt-get update
CMD ["echo" , "Join CloudNativeFolks Community"]
```
#### build dockerfile 

```sh
 docker build -t sangam14/cmd-dockerfile  -f dockerfile.cmd . 
[+] Building 8.3s (7/7) FINISHED                                                                                                                                                                  
 =&gt; [internal] load build definition from dockerfile.cmd                                                                                                                                     0.0s
 =&gt; =&gt; transferring dockerfile: 125B                                                                                                                                                         0.0s
 =&gt; [internal] load .dockerignore                                                                                                                                                            0.0s
 =&gt; =&gt; transferring context: 2B                                                                                                                                                              0.0s
 =&gt; [internal] load metadata for docker.io/library/ubuntu:latest                                                                                                                             2.1s
 =&gt; [auth] library/ubuntu:pull token for registry-1.docker.io                                                                                                                                0.0s
 =&gt; CACHED [1/2] FROM docker.io/library/ubuntu@sha256:9a0bdde4188b896a372804be2384015e90e3f84906b750c1a53539b585fbbe7f                                                                       0.0s
 =&gt; [2/2] RUN apt-get update                                                                                                                                                                 6.0s
 =&gt; exporting to image                                                                                                                                                                       0.1s
 =&gt; =&gt; exporting layers                                                                                                                                                                      0.1s
 =&gt; =&gt; writing image sha256:c59a693968aab28243f6852d49be7299e0035e71e39b42f22c07be49cca74fb2                                                                                                 0.0s
 =&gt; =&gt; naming to docker.io/sangam14/cmd-dockerfile 

```

#### run docker container 

```sh
Dockerfile git:(main) ✗ docker run sangam14/cmd-dockerfile 
Join CloudNativeFolks Community

```


## Dockerfile Entrypoint


#### Running a Docker Container with ENTRYPOINT

Let's learn the details in this case by actually executing ENTRYPOINT in exec form. The following is an example of a Dockerfile that uses the exec form of ENTRYPOINT, which outputs a character string on the command line.

```dockerfile 
FROM alpine
ENTRYPOINT ["echo", "Hello!"]

```

#### Build dockerfile 

```sh
➜  Dockerfile git:(main) ✗ docker build -t sangam14/entrypoint-dockerfile  -f dockerfile.entrypoint .
[+] Building 3.2s (6/6) FINISHED                                                                                            
 =&gt; [internal] load build definition from dockerfile.entrypoint                                                        0.0s
 =&gt; =&gt; transferring dockerfile: 94B                                                                                    0.0s
 =&gt; [internal] load .dockerignore                                                                                      0.0s
 =&gt; =&gt; transferring context: 2B                                                                                        0.0s
 =&gt; [internal] load metadata for docker.io/library/alpine:latest                                                       3.1s
 =&gt; [auth] library/alpine:pull token for registry-1.docker.io                                                          0.0s
 =&gt; [1/1] FROM docker.io/library/alpine@sha256:69665d02cb32192e52e07644d76bc6f25abeb5410edc1c7a81a10ba3f0efb90a        0.0s
 =&gt; =&gt; resolve docker.io/library/alpine@sha256:69665d02cb32192e52e07644d76bc6f25abeb5410edc1c7a81a10ba3f0efb90a        0.0s
 =&gt; =&gt; sha256:69665d02cb32192e52e07644d76bc6f25abeb5410edc1c7a81a10ba3f0efb90a 1.64kB / 1.64kB                         0.0s
 =&gt; =&gt; sha256:c41ab5c992deb4fe7e5da09f67a8804a46bd0592bfdf0b1847dde0e0889d2bff 528B / 528B                             0.0s
 =&gt; =&gt; sha256:d74e625d91152966d38fe8a62c60daadb96d4b94c1a366de01fab5f334806239 1.49kB / 1.49kB                         0.0s
 =&gt; exporting to image                                                                                                 0.0s
 =&gt; =&gt; exporting layers                                                                                                0.0s
 =&gt; =&gt; writing image sha256:0d5a798a648339f8ea8094b10568eb2dc44540480deff55f680dfd689f787013                           0.0s
 =&gt; =&gt; naming to docker.io/sangam14/entrypoint-dockerfile                                                              0.0s
 ```

#### run docker container 

```sh
 Dockerfile git:(main) ✗ docker run sangam14/entrypoint-dockerfile 
Hello!
➜  Dockerfile git:(main) ✗ docker run sangam14/entrypoint-dockerfile echo "sangam"
Hello! echo sangam
➜  Dockerfile git:(main) ✗ 

```

#### Overwrite with `--entrypoint` option

On the other hand, in ENTRYPOINT, you can change the instruction by using the option of `—entrypoint` as follows.

```sh
docker run --rm --entrypoint sh sangam14/entrypoint-dockerfile  -c 'echo "test"'
test

```

CMD and ENTRYPOINT have similar roles and are confusing, but they have different functions. CMD, ENTRYPOINT, and ENTRYPOINT also behave differently between shell form and exec form, so it's a good idea to use each function properly. The instructions in the Dockerfile are a bit complicated, but you can use them effectively if you understand them.



## Dockerfile WORKDIR



The WORKDIR command is used to define the working directory of a Docker container at any given time. The command is specified in the Dockerfile.

Any RUN, CMD, ADD, COPY, or ENTRYPOINT command will be executed in the specified working directory.

# WORKDIR instruction Dockerfile for Docker Quick Start

```Dockerfile
FROM ubuntu
WORKDIR /var/www/html
RUN apt-get update &amp;&amp; apt-get install -y nginx
COPY index.html .
ENTRYPOINT ["nginx", "-g", "daemon off;"]
```
#### build dockerfile 

```sh
docker build -t sangam14/workdir-dockerfile  -f dockerfile.workdir .

```
#### run docker container 

```sh
docker run -p 80:80 sangam14/workdir-dockerfile 

```
output 

![](./images/ngnix.png)


##  Dockerfile RUN 


The RUN command is the central executing directive for Dockerfiles. It takes a command as its argument and runs it to form the image. Unlike CMD, it actually is used to build the image (forming another layer on top of the previous one which is committed).


#### create dockerfile with following content 

```dockerfile
FROM ubuntu
RUN id
RUN useradd --create-home -m -s /bin/bash dev
# Add a fun prompt for dev user of my-app
# whale: "\xF0\x9F\x90\xB3"
# alien:"\xF0\x9F\x91\xBD"
# fish:"\xF0\x9F\x90\xA0"
# elephant:"\xF0\x9F\x91\xBD"
# moneybag:"\xF0\x9F\x92\xB0"
RUN echo 'PS1="\[$(tput bold)$(tput setaf 4)\]my-app $(echo -e "\xF0\x9F\x90\xB3") \[$(tput sgr0)\] [\\u@\\h]:\\W \\$ "' &gt;&gt; /home/dev/.bashrc &amp;&amp; \
    echo 'alias ls="ls --color=auto"' &gt;&gt; /home/dev/.bashrc

RUN mkdir /myvol
RUN echo "hello DQS Guide" &gt; /myvol/greeting
RUN ["chmod", "664", "/myvol/greeting"]
RUN ["chown", "dev:dev", "/myvol/greeting"]
VOLUME /myvol

USER dev
RUN id

CMD ["/bin/bash"]

```

### build docker container 

```bash
 ➜  docker build -t  sangam14/run-dockerfile -f dockerfile.run .
```


#### run docer container 

```sh
➜  Dockerfile git:(main) ✗ docker run -it  sangam14/run-dockerfile 
```
#### added user as my-app with whale emoji 

```sh
my-app 🐳  [dev@0270ab5e6f0c]:/ $ ls
bin  boot  dev  etc  home  lib  media  mnt  myvol  opt  proc  root  run  sbin  srv  sys  tmp  usr  var
my-app 🐳  [dev@0270ab5e6f0c]:/ $ cat myvol/greeting 
hello DQS Guide

```

# Dockerfile ARG 


```dockerfile 
FROM alpine

ENV key1="ENV is stronger than an ARG"
RUN echo ${key1}
ARG key1="not going to matter"
RUN echo ${key1}

RUN echo ${key2}
ARG key2="defaultValue"
RUN echo ${key2}
ENV key2="ENV value takes over"
RUN echo ${key2}
CMD ["sh"]
```

#### Build Dockerfile 

```sh
 Dockerfile git:(main) ✗ docker build -t sangam14/arg-dockerfile  -f dockerfile.arg .
[+] Building 3.5s (11/11) FINISHED                                                                                                                         
 =&gt; [internal] load build definition from dockerfile.arg                                                                                              0.0s
 =&gt; =&gt; transferring dockerfile: 336B                                                                                                                  0.0s
 =&gt; [internal] load .dockerignore                                                                                                                     0.0s
 =&gt; =&gt; transferring context: 2B                                                                                                                       0.0s
 =&gt; [internal] load metadata for docker.io/library/alpine:latest                                                                                      2.2s
 =&gt; [auth] library/alpine:pull token for registry-1.docker.io                                                                                         0.0s
 =&gt; CACHED [1/6] FROM docker.io/library/alpine@sha256:69665d02cb32192e52e07644d76bc6f25abeb5410edc1c7a81a10ba3f0efb90a                                0.0s
 =&gt; [2/6] RUN echo ENV is stronger than an ARG                                                                                                        0.2s
 =&gt; [3/6] RUN echo not going to matter                                                                                                                0.1s
 =&gt; [4/6] RUN echo ${key2}                                                                                                                            0.3s
 =&gt; [5/6] RUN echo defaultValue                                                                                                                       0.2s
 =&gt; [6/6] RUN echo ENV value takes over                                                                                                               0.3s
 =&gt; exporting to image                                                                                                                                0.0s
 =&gt; =&gt; exporting layers                                                                                                                               0.0s
 =&gt; =&gt; writing image sha256:acf55f3ef13e44ff24acf18f9c6320e5af33aa3eb9789274a46f47a9dff6d474                                                          0.0s
 =&gt; =&gt; naming to docker.io/sangam14/arg-dockerfile             

 ```

#### Inspect Env variable 

```sh
docker image inspect --format '{{json .Config}}' sangam14/arg-demo:1.0 | jq '.Env'

```

output 

```sh
docker image inspect --format '{{json .Config}}' sangam14/arg-dockerfile | jq '.Env'
[
  "PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin",
  "key1=ENV is stronger than an ARG",
  "key2=ENV value takes over"
]
```
docker container run sangam14/arg-dockerfile env  

```sh
docker container run sangam14/arg-dockerfile env                                    
PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
HOSTNAME=7b09d8fffd50
key1=ENV is stronger than an ARG
key2=ENV value takes over
HOME=/root

```
#### Pass env values while building dockerfile 

```sh
docker  build --rm --build-arg key1="buildTimeValue" --build-arg key2="good till env instruction" --tag sangam14/arg-dockerfile1 -f dockerfile.arg . 
Sending build context to Docker daemon  50.18kB
Step 1/11 : FROM alpine
latest: Pulling from library/alpine
af6eaf76a39c: Already exists 
Digest: sha256:69665d02cb32192e52e07644d76bc6f25abeb5410edc1c7a81a10ba3f0efb90a
Status: Downloaded newer image for alpine:latest
 ]]></description>
      
    </item>
    <item>
      <title>undefined</title>
      <link>https://kubernetesdaily.github.io/labs/labs/Learn-Containerd</link>
      <guid isPermaLink="true">https://kubernetesdaily.github.io/labs/labs/Learn-Containerd</guid>
      <pubDate>Tue, 29 Apr 2025 06:45:07 GMT</pubDate>
      <description><![CDATA[

### History of ContainerD 


**Origin from Docker:** The origins of containerd can be traced back to Docker, the popular container platform. Docker initially included all components required to run a container in a single monolithic binary, which included the container runtime. As Docker grew, there was a need to break down this monolithic structure into more manageable, modular components.

**Announcement and Spin-Off (2016):** Docker announced containerd in December 2016 as a core component split off from the Docker Engine. This move was part of Docker's effort to modularize its platform. containerd was designed to manage the entire container lifecycle, including image transfer and storage, container execution and supervision, low-level storage, and network attachments.
Donation to CNCF (2017): In March 2017, Docker donated containerd to the Cloud Native Computing Foundation (CNCF), the same foundation that hosts Kubernetes. This move was aimed at fostering an open, neutral, and community-driven base for container operations.

**Graduation from CNCF (2019):** containerd graduated from CNCF in February 2019, which indicated its maturity, stable API, and growing adoption. Graduation from CNCF is a significant milestone that reflects a project’s sustainability and adherence to certain standards of governance.


**Adoption and Usage:** Over the years, containerd has seen significant adoption in the industry. It is used by Kubernetes, as well as cloud providers and Linux distributions for their container operations. This wide adoption is due to its simplicity, robustness, and the strong community support it has garnered.



![](https://containerd.io/img/architecture.png)

This diagram represents the architecture of containerd, an industry-standard core container runtime. It is designed to be less opinionated and can be used as a base for building a more complex container platform. Let's break down the architecture depicted in the image:

**Ecosystem Layer**

The topmost layer is the ecosystem that interacts with containerd. It includes:

- Platforms: Cloud platforms and container orchestration systems that use containerd as their runtime, such as Google Cloud Platform, Docker, IBM Cloud, Microsoft Azure, Alibaba Cloud, AWS, and others.

- Clients: Tools that interact with containerd, like kubectl for Kubernetes, Docker CLI, ctr (containerd's own CLI tool), BuildKit for building containers, and others.

- CRI Runtime: containerd is compatible with the Kubernetes Container Runtime Interface (CRI), making it possible to use containerd as the runtime in Kubernetes.


**containerd Layer**

This is the core of containerd, divided into several components:

- API: The gRPC API layer through which clients communicate with containerd. It defines service handlers for various container management tasks.

- Service Handlers: These are gRPC services that handle API requests. They include services for managing containers, images, snapshots, tasks, namespaces, and more.

**Core Layer**

This is where the main functionality of containerd resides:

- Services: These are the backend services that do the actual work. They include:
Containers Service: Manages container metadata.

- Content Service: Manages the storage and retrieval of content like layers and config.

- Diff Service: Handles layer diff computations.

- Images Service: Manages image metadata.
Leases Service: Manages client leases for resources.

- Namespaces Service: Provides multi-tenancy by segregating resources per namespace.

- Snapshots Service: Manages filesystem snapshots.

- Tasks Service: Manages the execution of containers.

**Backend Layer**

The backend consists of the components that interact with the host system:

- Content Store: Storage for content like container images. This can be local storage or a plugin.

- Snapshotter: Handles the creation of filesystem snapshots. There are various snapshotter plugins available, including overlay, btrfs, devmapper, native, and windows.

- Runtime: This is where the containers are actually run. containerd supports multiple runtimes:

- runc: The default OCI runtime for Linux containers.

- runhcs: A runtime for running Windows containers.

- Kata Containers: Provides lightweight VMs that offer more isolation than traditional container environments.

- Firecracker: A microVM manager for creating and managing microVMs.
gVisor: A user-space kernel for providing sandboxed environments.

- V2 shim client: A newer version of the shim that manages the lifecycle of the containers and the host processes related to it.

**System Layer**

The bottom layer represents the system dependencies:

- Operating Systems: The diagram shows containerd being compatible with ARM and Intel architectures, indicating cross-platform support.

- Linux Penguins: Symbolizes that containerd is a Linux-native technology, but it also supports Windows as indicated by the "windows" snapshotter.
Metrics Off to the side, there is a connection to Prometheus, which is a monitoring system that collects metrics. containerd has built-in support for exporting metrics that Prometheus can scrape and use for monitoring and alerting.

- Overall, containerd is designed to handle the lifecycle of containers and images in a container system, manage storage and execution, and allow for extensive customization through plugins and external tooling. It is a key component in many modern container platforms, offering a balance between functionality and flexibility.


#### install ruc 


```
sangam@sangam:~$ sudo apt-get -y install runc 
Reading package lists... Done
Building dependency tree... Done
Reading state information... Done
runc is already the newest version (1.1.7-0ubuntu1~22.04.2).
0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.


sangam@sangam:~$ runc 
NAME:
   runc - Open Container Initiative runtime

runc is a command line client for running applications packaged according to
the Open Container Initiative (OCI) format and is a compliant implementation of the
Open Container Initiative specification.

runc integrates well with existing process supervisors to provide a production
container runtime environment for applications. It can be used with your
existing process monitoring tools and the container will be spawned as a
direct child of the process supervisor.

Containers are configured using bundles. A bundle for a container is a directory
that includes a specification file named "config.json" and a root filesystem.
The root filesystem contains the contents of the container.

To start a new instance of a container:

    # runc run [ -b bundle ] &lt;container-id&gt;

Where "&lt;container-id&gt;" is your name for the instance of the container that you
are starting. The name you provide for the container instance must be unique on
your host. Providing the bundle directory using "-b" is optional. The default
value for "bundle" is the current directory.

USAGE:
   runc [global options] command [command options] [arguments...]

VERSION:
   1.1.7-0ubuntu1~22.04.2
spec: 1.0.2-dev
go: go1.18.1
libseccomp: 2.5.3

COMMANDS:
   checkpoint  checkpoint a running container
   create      create a container
   delete      delete any resources held by the container often used with detached container
   events      display container events such as OOM notifications, cpu, memory, and IO usage statistics
   exec        execute new process inside the container
   kill        kill sends the specified signal (default: SIGTERM) to the container's init process
   list        lists containers started by runc with the given root
   pause       pause suspends all processes inside the container
   ps          ps displays the processes running inside a container
   restore     restore a container from a previous checkpoint
   resume      resumes all processes that have been previously paused
   run         create and run a container
   spec        create a new specification file
   start       executes the user defined process in a created container
   state       output the state of a container
   update      update container resource constraints
   features    show the enabled features
   help, h     Shows a list of commands or help for one command

GLOBAL OPTIONS:
   --debug             enable debug logging
   --log value         set the log file to write runc logs to (default is '/dev/stderr')
   --log-format value  set the log format ('text' (default), or 'json') (default: "text")
   --root value        root directory for storage of container state (this should be located in tmpfs) (default: "/run/user/1000/runc")
   --criu value        path to the criu binary used for checkpoint and restore (default: "criu")
   --systemd-cgroup    enable systemd cgroup support, expects cgroupsPath to be of form "slice:prefix:name" for e.g. "system.slice:runc:434234"
   --rootless value    ignore cgroup permission errors ('true', 'false', or 'auto') (default: "auto")
   --help, -h          show help
   --version, -v       print the version
sangam@sangam:~$ 


```

### create directory 
```
sangam@sangam:~$ mkdir runc-sangam-demo
sangam@sangam:~$ cd runc-sangam-demo/
```


once installed, you can start using runc:

### Creating a Container

Create a Root Filesystem: You need a root filesystem for your container. You can create a simple one or use an existing Docker image.
Example: mkdir rootfs and unpack a Docker image into it using docker export and tar.
```
mkdir rootfs
```

### Export the filesystem of an existing Docker image (e.g., alphine)

```
sudo docker pull alpine
Using default tag: latest
latest: Pulling from library/alpine
bca4290a9639: Pull complete 
Digest: sha256:c5b1261d6d3e43071626931fc004f70149baeba2c8ec672bd4f27761f8e1ad6b
Status: Downloaded newer image for alpine:latest
docker.io/library/alpine:latest

docker export $(docker create alpine) | tar -C rootfs -xvf -


```

### it will give us two files 

```
sangam@sangam:~/runc-sangam-demo$ ls
config.json  rootfs

```

### check it out config.json file 


```
sangam@sangam:~/runc-sangam-demo$ cat config.json 
{
	"ociVersion": "1.0.2-dev",
	"process": {
		"terminal": true,
		"user": {
			"uid": 0,
			"gid": 0
		},
		"args": [
			"sh"
		],
		"env": [
			"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin",
			"TERM=xterm"
		],
		"cwd": "/",
		"capabilities": {
			"bounding": [
				"CAP_AUDIT_WRITE",
				"CAP_KILL",
				"CAP_NET_BIND_SERVICE"
			],
			"effective": [
				"CAP_AUDIT_WRITE",
				"CAP_KILL",
				"CAP_NET_BIND_SERVICE"
			],
			"permitted": [
				"CAP_AUDIT_WRITE",
				"CAP_KILL",
				"CAP_NET_BIND_SERVICE"
			],
			"ambient": [
				"CAP_AUDIT_WRITE",
				"CAP_KILL",
				"CAP_NET_BIND_SERVICE"
			]
		},
		"rlimits": [
			{
				"type": "RLIMIT_NOFILE",
				"hard": 1024,
				"soft": 1024
			}
		],
		"noNewPrivileges": true
	},
	"root": {
		"path": "rootfs",
		"readonly": true
	},
	"hostname": "runc",
	"mounts": [
		{
			"destination": "/proc",
			"type": "proc",
			"source": "proc"
		},
		{
			"destination": "/dev",
			"type": "tmpfs",
			"source": "tmpfs",
			"options": [
				"nosuid",
				"strictatime",
				"mode=755",
				"size=65536k"
			]
		},
		{
			"destination": "/dev/pts",
			"type": "devpts",
			"source": "devpts",
			"options": [
				"nosuid",
				"noexec",
				"newinstance",
				"ptmxmode=0666",
				"mode=0620",
				"gid=5"
			]
		},
		{
			"destination": "/dev/shm",
			"type": "tmpfs",
			"source": "shm",
			"options": [
				"nosuid",
				"noexec",
				"nodev",
				"mode=1777",
				"size=65536k"
			]
		},
		{
			"destination": "/dev/mqueue",
			"type": "mqueue",
			"source": "mqueue",
			"options": [
				"nosuid",
				"noexec",
				"nodev"
			]
		},
		{
			"destination": "/sys",
			"type": "sysfs",
			"source": "sysfs",
			"options": [
				"nosuid",
				"noexec",
				"nodev",
				"ro"
			]
		},
		{
			"destination": "/sys/fs/cgroup",
			"type": "cgroup",
			"source": "cgroup",
			"options": [
				"nosuid",
				"noexec",
				"nodev",
				"relatime",
				"ro"
			]
		}
	],
	"linux": {
		"resources": {
			"devices": [
				{
					"allow": false,
					"access": "rwm"
				}
			]
		},
		"namespaces": [
			{
				"type": "pid"
			},
			{
				"type": "network"
			},
			{
				"type": "ipc"
			},
			{
				"type": "uts"
			},
			{
				"type": "mount"
			},
			{
				"type": "cgroup"
			}
		],
		"maskedPaths": [
			"/proc/acpi",
			"/proc/asound",
			"/proc/kcore",
			"/proc/keys",
			"/proc/latency_stats",
			"/proc/timer_list",
			"/proc/timer_stats",
			"/proc/sched_debug",
			"/sys/firmware",
			"/proc/scsi"
		],
		"readonlyPaths": [
			"/proc/bus",
			"/proc/fs",
			"/proc/irq",
			"/proc/sys",
			"/proc/sysrq-trigger"
		]
	}
}
```


#### config file in depth 

**ociVersion**

ociVersion: Specifies the OCI Runtime Specification version that the configuration is compatible with.

**process**

This section details how the process inside the container should be run.

- terminal: If set to true, a terminal is attached to the process.
- user: Specifies the UID and GID for the process.
- args: Arguments for the process to be run inside the container. In your case, it starts a shell (sh).
- env: Environment variables.
- cwd: Current working directory inside the container.
- capabilities: Linux capabilities that the process inside the container can use.
- rlimits: Resource limits for the process (e.g., number of open files).
- noNewPrivileges: If true, the process is not allowed to gain new privileges.

**root** 

- path: The path to the root filesystem relative to the location of the config.json file. Here, it's set to use the rootfs directory.

- readonly: If true, the root filesystem is mounted as read-only.

**hostname**

- hostname: The hostname set for the container.

**mounts**

This section defines filesystems or directories that are mounted in the container.

- destination: Where the mount will be placed inside the container.
- type: The type of the filesystem to be mounted (e.g., proc, tmpfs).
- source: The source of the mount.
- options: Mount options.

**linux**

This section contains Linux-specific configurations.

- resources: Controls how the container can interact with system resources.
- namespaces: Namespaces provide isolation for the container. Common namespaces include pid, network, ipc, uts, mount, and cgroup.
- maskedPaths: Paths that should be hidden from the container.
- readonlyPaths: Paths that should be read-only inside the container.


#### runc run - create and run a container

```
runc spec --help
NAME:
   runc spec - create a new specification file

USAGE:
   runc spec [command options] [arguments...]

DESCRIPTION:
   The spec command creates the new specification file named "config.json" for
the bundle.

The spec generated is just a starter file. Editing of the spec is required to
achieve desired results. For example, the newly generated spec includes an args
parameter that is initially set to call the "sh" command when the container is
started. Calling "sh" may work for an ubuntu container or busybox, but will not
work for containers that do not include the "sh" program.

EXAMPLE:
  To run docker's hello-world container one needs to set the args parameter
in the spec to call hello. This can be done using the sed command or a text
editor. The following commands create a bundle for hello-world, change the
default args parameter in the spec from "sh" to "/hello", then run the hello
command in a new hello-world container named container1:

    mkdir hello
    cd hello
    docker pull hello-world
    docker export $(docker create hello-world) &gt; hello-world.tar
    mkdir rootfs
    tar -C rootfs -xf hello-world.tar
    runc spec
    sed -i 's;"sh";"/hello";' config.json
    runc run container1

In the run command above, "container1" is the name for the instance of the
container that you are starting. The name you provide for the container instance
must be unique on your host.

An alternative for generating a customized spec config is to use "oci-runtime-tool", the
sub-command "oci-runtime-tool generate" has lots of options that can be used to do any
customizations as you want, see runtime-tools (https://github.com/opencontainers/runtime-tools)
to get more information.

When starting a container through runc, runc needs root privilege. If not
already running as root, you can use sudo to give runc root privilege. For
example: "sudo runc start container1" will give runc root privilege to start the
container on your host.

Alternatively, you can start a rootless container, which has the ability to run
without root privileges. For this to work, the specification file needs to be
adjusted accordingly. You can pass the parameter --rootless to this command to
generate a proper rootless spec file.

Note that --rootless is not needed when you execute runc as the root in a user namespace
created by an unprivileged user.


OPTIONS:
   --bundle value, -b value  path to the root of the bundle directory
   --rootless                generate a configuration for a rootless container
   
```

### lets create 

```
sangam@sangam:~/runc-sangam-demo$ sudo runc create --bundle . container1
sangam@sangam:~/runc-sangam-demo$ sudo runc list
ID           PID         STATUS      BUNDLE                          CREATED                         OWNER
container1   379391      created     /home/sangam/runc-sangam-demo   2024-02-08T16:08:52.31926604Z   root

```

### list all namespaces 

```

sangam@sangam:~/runc-sangam-demo$ sudo lsns
        NS TYPE   NPROCS    PID USER             COMMAND
4026531834 time      221      1 root             /sbin/init
4026531835 cgroup    220      1 root             /sbin/init
4026531836 pid       208      1 root             /sbin/init
4026531837 user      221      1 root             /sbin/init
4026531838 uts       217      1 root             /sbin/init
4026531839 ipc       208      1 root             /sbin/init
4026531840 net       220      1 root             /sbin/init
4026531841 mnt       202      1 root             /sbin/init
4026531862 mnt         1     25 root             kdevtmpfs
4026532545 mnt         1    561 root             /lib/systemd/systemd-udevd
4026532546 uts         1    561 root             /lib/systemd/systemd-udevd
4026532626 mnt         1    714 systemd-timesync /lib/systemd/systemd-timesyncd
4026532627 uts         1    714 systemd-timesync /lib/systemd/systemd-timesyncd
4026532628 mnt         1    821 systemd-network  /lib/systemd/systemd-networkd
4026532629 mnt         1    823 systemd-resolve  /lib/systemd/systemd-resolved
4026532660 mnt         1   2373 65535            /pause
4026532661 ipc        12   2373 65535            /pause
4026532662 pid         1   2373 65535            /pause
4026532663 mnt        11   2662 root             /usr/local/bin/runsvdir -P /etc/service/enabled
4026532664 pid        11   2662 root             /usr/local/bin/runsvdir -P /etc/service/enabled
4026532666 mnt         1 379391 root             runc init
4026532667 uts         1 379391 root             runc init
4026532668 ipc         1 379391 root             runc init
4026532669 pid         1 379391 root             runc init
4026532670 net         1 379391 root             runc init
4026532713 mnt         1    869 root             /lib/systemd/systemd-logind
4026532714 uts         1    869 root             /lib/systemd/systemd-logind
4026532732 cgroup      1 379391 root             runc init

```

### list of all running processes on your system, including details about the user running each process, CPU and memory usage, the process ID (PID), the parent process ID (PPID), and more.

```
sangam@sangam:~/runc-sangam-demo$ ps auxf
USER         PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
root           2  0.0  0.0      0     0 ?        S    11:32   0:00 [kthreadd]
root           3  0.0  0.0      0     0 ?        I&lt;   11:32   0:00  \_ [rcu_gp]
root           4  0.0  0.0      0     0 ?        I&lt;   11:32   0:00  \_ [rcu_par_gp]
root           5  0.0  0.0      0     0 ?        I&lt;   11:32   0:00  \_ [slub_flushwq]
root           6  0.0  0.0      0     0 ?        I&lt;   11:32   0:00  \_ [netns]
root           8  0.0  0.0      0     0 ?        I&lt;   11:32   0:00  \_ [kworker/0:0H-events_highpri]
root          10  0.0  0.0      0     0 ?        I&lt;   11:32   0:00  \_ [mm_percpu_wq]
root          11  0.0  0.0      0     0 ?        S    11:32   0:00  \_ [rcu_tasks_rude_]
root          12  0.0  0.0      0     0 ?        S    11:32   0:00  \_ [rcu_tasks_trace]
root          13  0.0  0.0      0     0 ?        S    11:32   0:01  \_ [ksoftirqd/0]
root          14  0.0  0.0      0     0 ?        I    11:32   0:06  \_ [rcu_sched]
root          15  0.0  0.0      0     0 ?        S    11:32   0:00  \_ [migration/0]
root          16  0.0  0.0      0     0 ?        S    11:32   0:00  \_ [idle_inject/0]
root          18  0.0  0.0      0     0 ?        S    11:32   0:00  \_ [cpuhp/0]
root          19  0.0  0.0      0     0 ?        S    11:32   0:00  \_ [cpuhp/1]
root          20  0.0  0.0      0     0 ?        S    11:32   0:00  \_ [idle_inject/1]
root          21  0.0  0.0      0     0 ?        S    11:32   0:00  \_ [migration/1]
root          22  0.0  0.0      0     0 ?        S    11:32   0:01  \_ [ksoftirqd/1]
root          24  0.0  0.0      0     0 ?        I&lt;   11:32   0:00  \_ [kworker/1:0H-events_highpri]
root          25  0.0  0.0      0     0 ?        S    11:32   0:00  \_ [kdevtmpfs]
root          26  0.0  0.0      0     0 ?        I&lt;   11:32   0:00  \_ [inet_frag_wq]
root          28  0.0  0.0      0     0 ?        S    11:32   0:00  \_ [kauditd]
root          29  0.0  0.0      0     0 ?        S    11:32   0:00  \_ [khungtaskd]
root          30  0.0  0.0      0     0 ?        S    11:32   0:00  \_ [oom_reaper]
root          31  0.0  0.0      0     0 ?        I&lt;   11:32   0:00  \_ [writeback]
root          32  0.0  0.0      0     0 ?        S    11:32   0:00  \_ [kcompactd0]
root          33  0.0  0.0      0     0 ?        SN   11:32   0:00  \_ [ksmd]
root          34  0.0  0.0      0     0 ?        SN   11:32   0:00  \_ [khugepaged]
root          80  0.0  0.0      0     0 ?        I&lt;   11:32   0:00  \_ [kintegrityd]
root          81  0.0  0.0      0     0 ?        I&lt;   11:32   0:00  \_ [kblockd]
root          82  0.0  0.0      0     0 ?        I&lt;   11:32   0:00  \_ [blkcg_punt_bio]
root          83  0.0  0.0      0     0 ?        I&lt;   11:32   0:00  \_ [tpm_dev_wq]
root          84  0.0  0.0      0     0 ?        I&lt;   11:32   0:00  \_ [ata_sff]
root          85  0.0  0.0      0     0 ?        I&lt;   11:32   0:00  \_ [md]
root          86  0.0  0.0      0     0 ?        I&lt;   11:32   0:00  \_ [edac-poller]
root          87  0.0  0.0      0     0 ?        I&lt;   11:32   0:00  \_ [devfreq_wq]
root          88  0.0  0.0      0     0 ?        S    11:32   0:00  \_ [watchdogd]
root          90  0.0  0.0      0     0 ?        I&lt;   11:32   0:00  \_ [kworker/0:1H-kblockd]
root          91  0.0  0.0      0     0 ?        S    11:32   0:00  \_ [kswapd0]
root          92  0.0  0.0      0     0 ?        S    11:32   0:00  \_ [ecryptfs-kthrea]
root          94  0.0  0.0      0     0 ?        I&lt;   11:32   0:00  \_ [kthrotld]
root          95  0.0  0.0      0     0 ?        S    11:32   0:00  \_ [irq/13-pciehp]
root          96  0.0  0.0      0     0 ?        S    11:32   0:00  \_ [irq/14-pciehp]
root          97  0.0  0.0      0     0 ?        S    11:32   0:00  \_ [irq/15-pciehp]
root          98  0.0  0.0      0     0 ?        S    11:32   0:00  \_ [irq/16-pciehp]
root          99  0.0  0.0      0     0 ?        S    11:32   0:00  \_ [irq/17-pciehp]
root         100  0.0  0.0      0     0 ?        S    11:32   0:00  \_ [irq/18-pciehp]
root         101  0.0  0.0      0     0 ?        S    11:32   0:00  \_ [irq/19-pciehp]
root         102  0.0  0.0      0     0 ?        S    11:32   0:00  \_ [irq/20-pciehp]
root         103  0.0  0.0      0     0 ?        S    11:32   0:00  \_ [irq/21-pciehp]
root         104  0.0  0.0      0     0 ?        S    11:32   0:00  \_ [irq/22-pciehp]
root         105  0.0  0.0      0     0 ?        S    11:32   0:00  \_ [irq/23-pciehp]
root         106  0.0  0.0      0     0 ?        S    11:32   0:00  \_ [irq/24-pciehp]
root         107  0.0  0.0      0     0 ?        S    11:32   0:00  \_ [irq/25-pciehp]
root         108  0.0  0.0      0     0 ?        S    11:32   0:00  \_ [irq/26-pciehp]
root         109  0.0  0.0      0     0 ?        S    11:32   0:00  \_ [irq/27-pciehp]
root         110  0.0  0.0      0     0 ?        S    11:32   0:00  \_ [irq/28-pciehp]
root         111  0.0  0.0      0     0 ?        S    11:32   0:00  \_ [irq/29-pciehp]
root         112  0.0  0.0      0     0 ?        S    11:32   0:00  \_ [irq/30-pciehp]
root         113  0.0  0.0      0     0 ?        S    11:32   0:00  \_ [irq/31-pciehp]
root         114  0.0  0.0      0     0 ?        S    11:32   0:00  \_ [irq/32-pciehp]
root         115  0.0  0.0      0     0 ?        S    11:32   0:00  \_ [irq/33-pciehp]
root         116  0.0  0.0      0     0 ?        S    11:32   0:00  \_ [irq/34-pciehp]
root         117  0.0  0.0      0     0 ?        S    11:32   0:00  \_ [irq/35-pciehp]
root         118  0.0  0.0      0     0 ?        S    11:32   0:00  \_ [irq/36-pciehp]
root         119  0.0  0.0      0     0 ?        S    11:32   0:00  \_ [irq/37-pciehp]
root         120  0.0  0.0      0     0 ?        S    11:32   0:00  \_ [irq/38-pciehp]
root         121  0.0  0.0      0     0 ?        S    11:32   0:00  \_ [irq/39-pciehp]
root         122  0.0  0.0      0     0 ?        S    11:32   0:00  \_ [irq/40-pciehp]
root         123  0.0  0.0      0     0 ?        S    11:32   0:00  \_ [irq/41-pciehp]
root         124  0.0  0.0      0     0 ?        S    11:32   0:00  \_ [irq/42-pciehp]
root         125  0.0  0.0      0     0 ?        S    11:32   0:00  \_ [irq/43-pciehp]
root         126  0.0  0.0      0     0 ?        S    11:32   0:00  \_ [irq/44-pciehp]
root         127  0.0  0.0      0     0 ?        I&lt;   11:32   0:00  \_ [acpi_thermal_pm]
root         129  0.0  0.0      0     0 ?        I&lt;   11:32   0:00  \_ [mld]
root         130  0.0  0.0      0     0 ?        I&lt;   11:32   0:00  \_ [ipv6_addrconf]
root         140  0.0  0.0      0     0 ?        I&lt;   11:32   0:00  \_ [kstrp]
root         144  0.0  0.0      0     0 ?        I&lt;   11:32   0:00  \_ [zswap-shrink]
root         145  0.0  0.0      0     0 ?        I&lt;   11:32   0:00  \_ [kworker/u5:0]
root         148  0.0  0.0      0     0 ?        I&lt;   11:32   0:00  \_ [cryptd]
root         187  0.0  0.0      0     0 ?        I&lt;   11:32   0:00  \_ [charger_manager]
root         211  0.0  0.0      0     0 ?        I&lt;   11:32   0:00  \_ [kworker/1:1H-kblockd]
root         234  0.0  0.0      0     0 ?        I&lt;   11:32   0:00  \_ [nvme-wq]
root         235  0.0  0.0      0     0 ?        I&lt;   11:32   0:00  \_ [nvme-reset-wq]
root         236  0.0  0.0      0     0 ?        I&lt;   11:32   0:00  \_ [nvme-delete-wq]
root         238  0.0  0.0      0     0 ?        S    11:32   0:00  \_ [scsi_eh_0]
root         239  0.0  0.0      0     0 ?        I&lt;   11:32   0:00  \_ [scsi_tmf_0]
root         240  0.0  0.0      0     0 ?        S    11:32   0:00  \_ [scsi_eh_1]
root         241  0.0  0.0      0     0 ?        I&lt;   11:32   0:00  \_ [scsi_tmf_1]
root         242  0.0  0.0      0     0 ?        S    11:32   0:00  \_ [scsi_eh_2]
root         243  0.0  0.0      0     0 ?        I&lt;   11:32   0:00  \_ [scsi_tmf_2]
root         244  0.0  0.0      0     0 ?        S    11:32   0:00  \_ [scsi_eh_3]
root         259  0.0  0.0      0     0 ?        I&lt;   11:32   0:00  \_ [scsi_tmf_3]
root         261  0.0  0.0      0     0 ?        S    11:32   0:00  \_ [scsi_eh_4]
root         265  0.0  0.0      0     0 ?        I&lt;   11:32   0:00  \_ [scsi_tmf_4]
root         267  0.0  0.0      0     0 ?        S    11:32   0:00  \_ [scsi_eh_5]
root         268  0.0  0.0      0     0 ?        I&lt;   11:32   0:00  \_ [scsi_tmf_5]
root         271  0.0  0.0      0     0 ?        S    11:32   0:00  \_ [scsi_eh_6]
root         273  0.0  0.0      0     0 ?        I&lt;   11:32   0:00  \_ [scsi_tmf_6]
root         276  0.0  0.0      0     0 ?        I&lt;   11:32   0:00  \_ [ttm_swap]
root         277  0.0  0.0      0     0 ?        S    11:32   0:00  \_ [scsi_eh_7]
root         279  0.0  0.0      0     0 ?        S    11:32   0:02  \_ [irq/45-vmwgfx]
root         281  0.0  0.0      0     0 ?        S    11:32   0:00  \_ [card0-crtc0]
root         282  0.0  0.0      0     0 ?        S    11:32   0:00  \_ [card0-crtc1]
root         283  0.0  0.0      0     0 ?        S    11:32   0:00  \_ [card0-crtc2]
root         284  0.0  0.0      0     0 ?        S    11:32   0:00  \_ [card0-crtc3]
root         285  0.0  0.0      0     0 ?        S    11:32   0:00  \_ [card0-crtc4]
root         286  0.0  0.0      0     0 ?        S    11:32   0:00  \_ [card0-crtc5]
root         287  0.0  0.0      0     0 ?        S    11:32   0:00  \_ [card0-crtc6]
root         288  0.0  0.0      0     0 ?        S    11:32   0:00  \_ [card0-crtc7]
root         289  0.0  0.0      0     0 ?        I&lt;   11:32   0:00  \_ [scsi_tmf_7]
root         293  0.0  0.0      0     0 ?        S    11:32   0:00  \_ [scsi_eh_8]
root         295  0.0  0.0      0     0 ?        I&lt;   11:32   0:00  \_ [scsi_tmf_8]
root         296  0.0  0.0      0     0 ?        S    11:32   0:00  \_ [scsi_eh_9]
root         298  0.0  0.0      0     0 ?        I&lt;   11:32   0:00  \_ [scsi_tmf_9]
root         300  0.0  0.0      0     0 ?        S    11:32   0:00  \_ [scsi_eh_10]
root         301  0.0  0.0      0     0 ?        I&lt;   11:32   0:00  \_ [scsi_tmf_10]
root         302  0.0  0.0      0     0 ?        S    11:32   0:00  \_ [scsi_eh_11]
root         304  0.0  0.0      0     0 ?        I&lt;   11:32   0:00  \_ [scsi_tmf_11]
root         305  0.0  0.0      0     0 ?        S    11:32   0:00  \_ [scsi_eh_12]
root         307  0.0  0.0      0     0 ?        I&lt;   11:32   0:00  \_ [scsi_tmf_12]
root         308  0.0  0.0      0     0 ?        S    11:32   0:00  \_ [scsi_eh_13]
root         310  0.0  0.0      0     0 ?        I&lt;   11:32   0:00  \_ [scsi_tmf_13]
root         315  0.0  0.0      0     0 ?        S    11:32   0:00  \_ [scsi_eh_14]
root         316  0.0  0.0      0     0 ?        I&lt;   11:32   0:00  \_ [scsi_tmf_14]
root         317  0.0  0.0      0     0 ?        S    11:32   0:00  \_ [scsi_eh_15]
root         319  0.0  0.0      0     0 ?        I&lt;   11:32   0:00  \_ [scsi_tmf_15]
root         321  0.0  0.0      0     0 ?        S    11:32   0:00  \_ [scsi_eh_16]
root         322  0.0  0.0      0     0 ?        I&lt;   11:32   0:00  \_ [scsi_tmf_16]
root         323  0.0  0.0      0     0 ?        S    11:32   0:00  \_ [scsi_eh_17]
root         324  0.0  0.0      0     0 ?        I&lt;   11:32   0:00  \_ [scsi_tmf_17]
root         325  0.0  0.0      0     0 ?        S    11:32   0:00  \_ [scsi_eh_18]
root         326  0.0  0.0      0     0 ?        I&lt;   11:32   0:00  \_ [scsi_tmf_18]
root         327  0.0  0.0      0     0 ?        S    11:32   0:00  \_ [scsi_eh_19]
root         328  0.0  0.0      0     0 ?        I&lt;   11:32   0:00  \_ [scsi_tmf_19]
root         329  0.0  0.0      0     0 ?        S    11:32   0:00  \_ [scsi_eh_20]
root         330  0.0  0.0      0     0 ?        I&lt;   11:32   0:00  \_ [scsi_tmf_20]
root         331  0.0  0.0      0     0 ?        S    11:32   0:00  \_ [scsi_eh_21]
root         332  0.0  0.0      0     0 ?        I&lt;   11:32   0:00  \_ [scsi_tmf_21]
root         333  0.0  0.0      0     0 ?        S    11:32   0:00  \_ [scsi_eh_22]
root         334  0.0  0.0      0     0 ?        I&lt;   11:32   0:00  \_ [scsi_tmf_22]
root         335  0.0  0.0      0     0 ?        S    11:32   0:00  \_ [scsi_eh_23]
root         336  0.0  0.0      0     0 ?        I&lt;   11:32   0:00  \_ [scsi_tmf_23]
root         337  0.0  0.0      0     0 ?        S    11:32   0:00  \_ [scsi_eh_24]
root         338  0.0  0.0      0     0 ?        I&lt;   11:32   0:00  \_ [scsi_tmf_24]
root         339  0.0  0.0      0     0 ?        S    11:32   0:00  \_ [scsi_eh_25]
root         340  0.0  0.0      0     0 ?        I&lt;   11:32   0:00  \_ [scsi_tmf_25]
root         341  0.0  0.0      0     0 ?        S    11:32   0:00  \_ [scsi_eh_26]
root         342  0.0  0.0      0     0 ?        I&lt;   11:32   0:00  \_ [scsi_tmf_26]
root         343  0.0  0.0      0     0 ?        S    11:32   0:00  \_ [scsi_eh_27]
root         344  0.0  0.0      0     0 ?        I&lt;   11:32   0:00  \_ [scsi_tmf_27]
root         345  0.0  0.0      0     0 ?        S    11:32   0:00  \_ [scsi_eh_28]
root         346  0.0  0.0      0     0 ?        I&lt;   11:32   0:00  \_ [scsi_tmf_28]
root         347  0.0  0.0      0     0 ?        S    11:32   0:00  \_ [scsi_eh_29]
root         348  0.0  0.0      0     0 ?        I&lt;   11:32   0:00  \_ [scsi_tmf_29]
root         382  0.0  0.0      0     0 ?        I&lt;   11:32   0:00  \_ [kdmflush]
root         416  0.0  0.0      0     0 ?        I&lt;   11:32   0:00  \_ [raid5wq]
root         463  0.0  0.0      0     0 ?        S    11:32   0:02  \_ [jbd2/dm-0-8]
root         464  0.0  0.0      0     0 ?        I&lt;   11:32   0:00  \_ [ext4-rsv-conver]
root         560  0.0  0.0      0     0 ?        I&lt;   11:32   0:00  \_ [kaluad]
root         562  0.0  0.0      0     0 ?        I&lt;   11:32   0:00  \_ [kmpath_rdacd]
root         564  0.0  0.0      0     0 ?        I&lt;   11:32   0:00  \_ [kmpathd]
root         565  0.0  0.0      0     0 ?        I&lt;   11:32   0:00  \_ [kmpath_handlerd]
root         686  0.0  0.0      0     0 ?        S    11:32   0:00  \_ [jbd2/nvme0n1p2-]
root         688  0.0  0.0      0     0 ?        I&lt;   11:32   0:00  \_ [ext4-rsv-conver]
root        1721  0.0  0.0   2184   764 ?        S    11:32   0:00  \_ bpfilter_umh
root        2070  0.0  0.0      0     0 ?        I&lt;   11:32   0:00  \_ [dio/dm-0]
root      156869  0.0  0.0      0     0 ?        I    13:25   0:01  \_ [kworker/u4:1-flush-253:0]
root      313679  0.0  0.0      0     0 ?        I    15:20   0:00  \_ [kworker/u4:3-events_unbound]
root      363789  0.0  0.0      0     0 ?        I    15:57   0:00  \_ [kworker/0:2-events]
root      365001  0.0  0.0      0     0 ?        I    15:58   0:00  \_ [kworker/u4:2-events_unbound]
root      370567  0.0  0.0      0     0 ?        I    16:02   0:00  \_ [kworker/1:0-events]
root      378476  0.0  0.0      0     0 ?        I    16:08   0:00  \_ [kworker/1:1-events]
root      378512  0.0  0.0      0     0 ?        I    16:08   0:00  \_ [kworker/0:1-events]
root      385797  0.0  0.0      0     0 ?        I    16:13   0:00  \_ [kworker/0:0-events]
root           1  0.0  0.2 101092 10708 ?        Ss   11:32   0:15 /sbin/init
root         526  0.3  8.8 540332 352704 ?       S&lt;s  11:32   1:03 /lib/systemd/systemd-journald
root         561  0.0  0.1  25320  5924 ?        Ss   11:32   0:00 /lib/systemd/systemd-udevd
root         566  0.0  0.6 289660 25664 ?        SLsl 11:32   0:01 /sbin/multipathd -d -s
systemd+     714  0.0  0.1  88652  6344 ?        Ssl  11:32   0:00 /lib/systemd/systemd-timesyncd
systemd+     821  0.0  0.1  16400  7700 ?        Ss   11:32   0:00 /lib/systemd/systemd-networkd
systemd+     823  0.0  0.2  25204 11580 ?        Ss   11:32   0:00 /lib/systemd/systemd-resolved
message+     843  0.0  0.1   8840  4240 ?        Ss   11:32   0:00 @dbus-daemon --system --address=systemd: --nofork --nopidfile --systemd-activation --syslog-on
root         857  0.0  0.4  29952 17636 ?        Ss   11:32   0:00 /usr/bin/python3 /usr/bin/networkd-dispatcher --run-startup-triggers
root         860  0.0  0.0   4956  3756 ?        Ss   11:32   0:07 /bin/bash /snap/microk8s/6227/apiservice-kicker
root      386729  0.0  0.0   2380   204 ?        S    16:14   0:00  \_ sleep 5
root         862  0.0  0.0   4288  3144 ?        Ss   11:32   0:00 /bin/bash /snap/microk8s/6227/run-cluster-agent-with-args
root        1398  0.0  0.6 741492 26780 ?        Sl   11:32   0:02  \_ /snap/microk8s/6227/bin/cluster-agent cluster-agent --bind 0.0.0.0:25000 --keyfile /var/sn
root         863  0.9  1.2 1560972 48280 ?       Ssl  11:32   2:37 /snap/microk8s/6227/bin/containerd --config /var/snap/microk8s/6227/args/containerd.toml --roo
root         866  5.5  5.9 1751344 237952 ?      Ssl  11:32  15:38 /snap/microk8s/6227/bin/k8s-dqlite --storage-dir=/var/snap/microk8s/6227/var/kubernetes/backen
root         867  0.0  0.7 1391976 29656 ?       Ssl  11:32   0:15 /usr/lib/snapd/snapd
root         869  0.0  0.1  48580  6924 ?        Ss   11:32   0:00 /lib/systemd/systemd-logind
root         872  0.1  0.9 1344056 38712 ?       Ssl  11:32   0:20 /usr/bin/containerd
root         925  0.0  0.0   2608   808 tty1     Ss+  11:32   0:00 /sbin/agetty -o -p -- \u --noclear tty1 linux
root        1017  0.0  0.1  15152  7992 ?        Ss   11:32   0:00 sshd: /usr/sbin/sshd -D [listener] 0 of 10-100 startups
root        1546  0.0  0.2  18384  9616 ?        Ss   11:32   0:00  \_ sshd: sangam [priv]
sangam      1868  0.0  0.1  18516  6568 ?        S    11:32   0:00      \_ sshd: sangam@pts/0
sangam      1869  0.0  0.0   4684  3744 pts/0    Ss   11:32   0:00          \_ -bash
sangam    386730  0.0  0.0   8136  3300 pts/0    R+   16:14   0:00              \_ ps auxf
root        1052  0.0  0.4 107044 19948 ?        Ssl  11:32   0:00 /usr/bin/python3 /usr/share/unattended-upgrades/unattended-upgrade-shutdown --wait-for-signal
root        1102  0.0  1.6 1370164 64656 ?       Ssl  11:32   0:02 /usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
root        1552  6.1  8.9 1055132 360408 ?      Ssl  11:32  17:28 /snap/microk8s/6227/kubelite --scheduler-args-file=/var/snap/microk8s/6227/args/kube-scheduler
sangam      1839  0.0  0.2  17340  9064 ?        Ss   11:32   0:00 /lib/systemd/systemd --user
sangam      1840  0.0  0.1 105132  5160 ?        S    11:32   0:00  \_ (sd-pam)
root        2298  0.1  0.2 719424 10152 ?        Sl   11:32   0:29 /snap/microk8s/6227/bin/containerd-shim-runc-v2 -namespace k8s.io -id e3cdeab7e3463dcd4c0d1f96
65535       2373  0.0  0.0    792     4 ?        Ss   11:32   0:00  \_ /pause
root        2662  0.0  0.0   1992   504 ?        Ss   11:32   0:00  \_ /usr/local/bin/runsvdir -P /etc/service/enabled
root        2738  0.0  0.0   1840   496 ?        Ss   11:32   0:00      \_ runsv monitor-addresses
root        2744  0.0  1.1 1224388 44816 ?       Sl   11:32   0:01      |   \_ calico-node -monitor-addresses
root        2739  0.0  0.0   1840   520 ?        Ss   11:32   0:00      \_ runsv node-status-reporter
root        2745  0.1  1.2 1224388 50936 ?       Sl   11:32   0:17      |   \_ calico-node -status-reporter
root        2740  0.0  0.0   1840   516 ?        Ss   11:32   0:00      \_ runsv felix
root        2747  2.0  1.4 1667548 58712 ?       Sl   11:32   5:51      |   \_ calico-node -felix
root        2741  0.0  0.0   1840   512 ?        Ss   11:32   0:00      \_ runsv cni
root        2743  0.0  1.1 1224388 44616 ?       Sl   11:32   0:00      |   \_ calico-node -monitor-token
root        2742  0.0  0.0   1840   540 ?        Ss   11:32   0:00      \_ runsv allocate-tunnel-addrs
root        2746  0.1  1.3 1298376 55568 ?       Sl   11:32   0:27          \_ calico-node -allocate-tunnel-addrs
root      379391  0.0  0.2 1007224 8808 ?        Ssl  16:08   0:00 runc init
```

#### start container and check state of container 


```
sangam@sangam:~/runc-sangam-demo$ sudo runc run container1
y
y ...

yy
```

#### kill the container 1 process 

sangam@sangam:~$ sudo runc kill container1  SIGKILL 



#### Cheatsheet - Runc Commands for Managing Containers

The following table provides an overview of various `runc` commands and their usage, demonstrated with an Alpine Linux container named `container1`.

| Command              | Description                                                      | Example Usage                         |
|]]></description>
      
    </item>
    <item>
      <title>undefined</title>
      <link>https://kubernetesdaily.github.io/labs/labs/CONTRIBUTING</link>
      <guid isPermaLink="true">https://kubernetesdaily.github.io/labs/labs/CONTRIBUTING</guid>
      <pubDate>Tue, 29 Apr 2025 06:45:07 GMT</pubDate>
      <description><![CDATA[]]></description>
      
    </item>
  </channel>
</rss>